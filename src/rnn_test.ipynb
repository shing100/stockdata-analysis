{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "# tensorflow init \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"DISPLAY\" not in os.environ:\n",
    "    # remove Travis CI Error\n",
    "    matplotlib.use('Agg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('조비수정.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train Parameters\n",
    "seq_length = 90\n",
    "data_dim = 43\n",
    "hidden_dim = 5\n",
    "output_dim = 1\n",
    "learning_rate = 0.005\n",
    "iterations = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open, High, Low, Volume, Close\n",
    "# xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
    "# xy = xy[::-1]  # reverse order (chronically ordered)\n",
    "# print(np.shape(xy))\n",
    "# data = pd.read_excel('stock.xlsx')\n",
    "data = pd.read_excel('조비수정.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2 = data\n",
    "# test = data.ix[:,:10]  열수정하는 방법\n",
    "# xy = np.dstack([data2[1],data2[2],data2[3],data2[4],data2[5],data2[6],data2[7],data2[8],data2[9],data2[10],data2[11],data2[12],data2[13],data2[14],data2[15],data2[16],data2[17],data2[18],data2[19],data2[20],data2[21],data2[22],data2[23],data2[24],data2[25],data2[26],data2[27],data2[28],data2[29],data2[30],data2[31],data2[32],data2[33],data2[34],data2[35],data2[36],data2[37],data2[38],data2[39],data2[40],data2[41],data2[42],data2[43]])\n",
    "xy = np.array(data2)\n",
    "xy = xy.reshape(-1,data_dim)\n",
    "\n",
    "\n",
    "xy = MinMaxScaler(xy)\n",
    "x = xy\n",
    "y = xy[:, [0]]  # Close as label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09791036])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a dataset\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(y) - seq_length):\n",
    "    _x = x[i:i + seq_length]\n",
    "    _y = y[i + seq_length]  # Next close price\n",
    "#     print(_x, \"->\", _y)\n",
    "    dataX.append(_x)\n",
    "    dataY.append(_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/test split \n",
    "train_size = int(len(dataY) * 0.99)\n",
    "test_size = len(dataY) - train_size\n",
    "trainX, testX = np.array(dataX[0:train_size]), np.array(\n",
    "    dataX[train_size:len(dataX)])\n",
    "trainY, testY = np.array(dataY[0:train_size]), np.array(\n",
    "    dataY[train_size:len(dataY)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input place holders\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# build a LSTM network\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "\n",
    "Y_pred = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost/loss\n",
    "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "targets = tf.placeholder(tf.float32, [None, 1])\n",
    "predictions = tf.placeholder(tf.float32, [None, 1])\n",
    "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 0] loss: 824.7437133789062\n",
      "[step: 1] loss: 474.87493896484375\n",
      "[step: 2] loss: 294.2059326171875\n",
      "[step: 3] loss: 226.38458251953125\n",
      "[step: 4] loss: 207.13803100585938\n",
      "[step: 5] loss: 195.65997314453125\n",
      "[step: 6] loss: 177.76121520996094\n",
      "[step: 7] loss: 153.31101989746094\n",
      "[step: 8] loss: 126.53548431396484\n",
      "[step: 9] loss: 101.78870391845703\n",
      "[step: 10] loss: 82.14453125\n",
      "[step: 11] loss: 69.01417541503906\n",
      "[step: 12] loss: 62.137203216552734\n",
      "[step: 13] loss: 59.880836486816406\n",
      "[step: 14] loss: 59.872154235839844\n",
      "[step: 15] loss: 59.81163024902344\n",
      "[step: 16] loss: 58.162193298339844\n",
      "[step: 17] loss: 54.43437194824219\n",
      "[step: 18] loss: 49.04769515991211\n",
      "[step: 19] loss: 42.9533805847168\n",
      "[step: 20] loss: 37.21433639526367\n",
      "[step: 21] loss: 32.670188903808594\n",
      "[step: 22] loss: 29.751562118530273\n",
      "[step: 23] loss: 28.448959350585938\n",
      "[step: 24] loss: 28.400463104248047\n",
      "[step: 25] loss: 29.046091079711914\n",
      "[step: 26] loss: 29.800336837768555\n",
      "[step: 27] loss: 30.20136833190918\n",
      "[step: 28] loss: 30.00307846069336\n",
      "[step: 29] loss: 29.196147918701172\n",
      "[step: 30] loss: 27.96295166015625\n",
      "[step: 31] loss: 26.589189529418945\n",
      "[step: 32] loss: 25.361310958862305\n",
      "[step: 33] loss: 24.478017807006836\n",
      "[step: 34] loss: 23.99806785583496\n",
      "[step: 35] loss: 23.836994171142578\n",
      "[step: 36] loss: 23.811004638671875\n",
      "[step: 37] loss: 23.710975646972656\n",
      "[step: 38] loss: 23.37848663330078\n",
      "[step: 39] loss: 22.755714416503906\n",
      "[step: 40] loss: 21.89324951171875\n",
      "[step: 41] loss: 20.9180965423584\n",
      "[step: 42] loss: 19.980064392089844\n",
      "[step: 43] loss: 19.19845199584961\n",
      "[step: 44] loss: 18.6280517578125\n",
      "[step: 45] loss: 18.252178192138672\n",
      "[step: 46] loss: 18.00098991394043\n",
      "[step: 47] loss: 17.78417205810547\n",
      "[step: 48] loss: 17.5250301361084\n",
      "[step: 49] loss: 17.184459686279297\n",
      "[step: 50] loss: 16.767757415771484\n",
      "[step: 51] loss: 16.31491470336914\n",
      "[step: 52] loss: 15.879267692565918\n",
      "[step: 53] loss: 15.504037857055664\n",
      "[step: 54] loss: 15.205283164978027\n",
      "[step: 55] loss: 14.967558860778809\n",
      "[step: 56] loss: 14.753183364868164\n",
      "[step: 57] loss: 14.520157814025879\n",
      "[step: 58] loss: 14.240625381469727\n",
      "[step: 59] loss: 13.911510467529297\n",
      "[step: 60] loss: 13.553512573242188\n",
      "[step: 61] loss: 13.199499130249023\n",
      "[step: 62] loss: 12.878719329833984\n",
      "[step: 63] loss: 12.60428237915039\n",
      "[step: 64] loss: 12.369523048400879\n",
      "[step: 65] loss: 12.153940200805664\n",
      "[step: 66] loss: 11.935081481933594\n",
      "[step: 67] loss: 11.700206756591797\n",
      "[step: 68] loss: 11.451692581176758\n",
      "[step: 69] loss: 11.204052925109863\n",
      "[step: 70] loss: 10.974565505981445\n",
      "[step: 71] loss: 10.773046493530273\n",
      "[step: 72] loss: 10.596558570861816\n",
      "[step: 73] loss: 10.431821823120117\n",
      "[step: 74] loss: 10.263555526733398\n",
      "[step: 75] loss: 10.08333683013916\n",
      "[step: 76] loss: 9.893583297729492\n",
      "[step: 77] loss: 9.704310417175293\n",
      "[step: 78] loss: 9.525678634643555\n",
      "[step: 79] loss: 9.361272811889648\n",
      "[step: 80] loss: 9.206761360168457\n",
      "[step: 81] loss: 9.054149627685547\n",
      "[step: 82] loss: 8.898122787475586\n",
      "[step: 83] loss: 8.739816665649414\n",
      "[step: 84] loss: 8.585420608520508\n",
      "[step: 85] loss: 8.441154479980469\n",
      "[step: 86] loss: 8.308599472045898\n",
      "[step: 87] loss: 8.183968544006348\n",
      "[step: 88] loss: 8.061464309692383\n",
      "[step: 89] loss: 7.93779182434082\n",
      "[step: 90] loss: 7.814305305480957\n",
      "[step: 91] loss: 7.695087432861328\n",
      "[step: 92] loss: 7.58318567276001\n",
      "[step: 93] loss: 7.4780473709106445\n",
      "[step: 94] loss: 7.3763837814331055\n",
      "[step: 95] loss: 7.275341987609863\n",
      "[step: 96] loss: 7.175000190734863\n",
      "[step: 97] loss: 7.078018665313721\n",
      "[step: 98] loss: 6.9869465827941895\n",
      "[step: 99] loss: 6.901878833770752\n",
      "[step: 100] loss: 6.820656776428223\n",
      "[step: 101] loss: 6.741162300109863\n",
      "[step: 102] loss: 6.66324520111084\n",
      "[step: 103] loss: 6.588367462158203\n",
      "[step: 104] loss: 6.517624855041504\n",
      "[step: 105] loss: 6.450320243835449\n",
      "[step: 106] loss: 6.3847503662109375\n",
      "[step: 107] loss: 6.319960594177246\n",
      "[step: 108] loss: 6.2565789222717285\n",
      "[step: 109] loss: 6.195794582366943\n",
      "[step: 110] loss: 6.13783597946167\n",
      "[step: 111] loss: 6.081799507141113\n",
      "[step: 112] loss: 6.026737213134766\n",
      "[step: 113] loss: 5.972665786743164\n",
      "[step: 114] loss: 5.920288562774658\n",
      "[step: 115] loss: 5.869905471801758\n",
      "[step: 116] loss: 5.821001052856445\n",
      "[step: 117] loss: 5.77281379699707\n",
      "[step: 118] loss: 5.725154399871826\n",
      "[step: 119] loss: 5.678424835205078\n",
      "[step: 120] loss: 5.632899284362793\n",
      "[step: 121] loss: 5.588376045227051\n",
      "[step: 122] loss: 5.54440975189209\n",
      "[step: 123] loss: 5.500924110412598\n",
      "[step: 124] loss: 5.458216190338135\n",
      "[step: 125] loss: 5.416534900665283\n",
      "[step: 126] loss: 5.375739097595215\n",
      "[step: 127] loss: 5.335563659667969\n",
      "[step: 128] loss: 5.295928001403809\n",
      "[step: 129] loss: 5.257021903991699\n",
      "[step: 130] loss: 5.218939781188965\n",
      "[step: 131] loss: 5.181586742401123\n",
      "[step: 132] loss: 5.144794464111328\n",
      "[step: 133] loss: 5.108547687530518\n",
      "[step: 134] loss: 5.072993755340576\n",
      "[step: 135] loss: 5.038190841674805\n",
      "[step: 136] loss: 5.004065036773682\n",
      "[step: 137] loss: 4.970526695251465\n",
      "[step: 138] loss: 4.937583923339844\n",
      "[step: 139] loss: 4.905301094055176\n",
      "[step: 140] loss: 4.873689651489258\n",
      "[step: 141] loss: 4.842688083648682\n",
      "[step: 142] loss: 4.812216758728027\n",
      "[step: 143] loss: 4.7823028564453125\n",
      "[step: 144] loss: 4.752967834472656\n",
      "[step: 145] loss: 4.724215507507324\n",
      "[step: 146] loss: 4.695979595184326\n",
      "[step: 147] loss: 4.668227195739746\n",
      "[step: 148] loss: 4.6409912109375\n",
      "[step: 149] loss: 4.614272594451904\n",
      "[step: 150] loss: 4.588053226470947\n",
      "[step: 151] loss: 4.562286853790283\n",
      "[step: 152] loss: 4.536964416503906\n",
      "[step: 153] loss: 4.512094020843506\n",
      "[step: 154] loss: 4.487668514251709\n",
      "[step: 155] loss: 4.463669776916504\n",
      "[step: 156] loss: 4.4400634765625\n",
      "[step: 157] loss: 4.416849136352539\n",
      "[step: 158] loss: 4.394036293029785\n",
      "[step: 159] loss: 4.3716044425964355\n",
      "[step: 160] loss: 4.349536418914795\n",
      "[step: 161] loss: 4.327823638916016\n",
      "[step: 162] loss: 4.306464195251465\n",
      "[step: 163] loss: 4.285454273223877\n",
      "[step: 164] loss: 4.264778137207031\n",
      "[step: 165] loss: 4.244423866271973\n",
      "[step: 166] loss: 4.224387168884277\n",
      "[step: 167] loss: 4.204658508300781\n",
      "[step: 168] loss: 4.18523645401001\n",
      "[step: 169] loss: 4.166110038757324\n",
      "[step: 170] loss: 4.147266864776611\n",
      "[step: 171] loss: 4.128711700439453\n",
      "[step: 172] loss: 4.110429286956787\n",
      "[step: 173] loss: 4.0924177169799805\n",
      "[step: 174] loss: 4.074670791625977\n",
      "[step: 175] loss: 4.057181358337402\n",
      "[step: 176] loss: 4.0399394035339355\n",
      "[step: 177] loss: 4.022951602935791\n",
      "[step: 178] loss: 4.006204128265381\n",
      "[step: 179] loss: 3.989687919616699\n",
      "[step: 180] loss: 3.973409414291382\n",
      "[step: 181] loss: 3.9573538303375244\n",
      "[step: 182] loss: 3.941518545150757\n",
      "[step: 183] loss: 3.925902843475342\n",
      "[step: 184] loss: 3.9105000495910645\n",
      "[step: 185] loss: 3.89530611038208\n",
      "[step: 186] loss: 3.880319595336914\n",
      "[step: 187] loss: 3.865528106689453\n",
      "[step: 188] loss: 3.8509387969970703\n",
      "[step: 189] loss: 3.8365368843078613\n",
      "[step: 190] loss: 3.822328567504883\n",
      "[step: 191] loss: 3.8083057403564453\n",
      "[step: 192] loss: 3.794464588165283\n",
      "[step: 193] loss: 3.7808024883270264\n",
      "[step: 194] loss: 3.767317771911621\n",
      "[step: 195] loss: 3.7540061473846436\n",
      "[step: 196] loss: 3.7408571243286133\n",
      "[step: 197] loss: 3.7278847694396973\n",
      "[step: 198] loss: 3.7150697708129883\n",
      "[step: 199] loss: 3.7024166584014893\n",
      "[step: 200] loss: 3.6899261474609375\n",
      "[step: 201] loss: 3.677586793899536\n",
      "[step: 202] loss: 3.6654000282287598\n",
      "[step: 203] loss: 3.653364896774292\n",
      "[step: 204] loss: 3.641481399536133\n",
      "[step: 205] loss: 3.629737615585327\n",
      "[step: 206] loss: 3.618137836456299\n",
      "[step: 207] loss: 3.6066782474517822\n",
      "[step: 208] loss: 3.5953598022460938\n",
      "[step: 209] loss: 3.584170341491699\n",
      "[step: 210] loss: 3.5731194019317627\n",
      "[step: 211] loss: 3.562197685241699\n",
      "[step: 212] loss: 3.5514044761657715\n",
      "[step: 213] loss: 3.540738582611084\n",
      "[step: 214] loss: 3.5302011966705322\n",
      "[step: 215] loss: 3.5197787284851074\n",
      "[step: 216] loss: 3.5094807147979736\n",
      "[step: 217] loss: 3.499302864074707\n",
      "[step: 218] loss: 3.4892351627349854\n",
      "[step: 219] loss: 3.4792847633361816\n",
      "[step: 220] loss: 3.4694488048553467\n",
      "[step: 221] loss: 3.459721088409424\n",
      "[step: 222] loss: 3.450101375579834\n",
      "[step: 223] loss: 3.440589189529419\n",
      "[step: 224] loss: 3.4311842918395996\n",
      "[step: 225] loss: 3.421876907348633\n",
      "[step: 226] loss: 3.4126760959625244\n",
      "[step: 227] loss: 3.403573751449585\n",
      "[step: 228] loss: 3.394566297531128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 229] loss: 3.3856592178344727\n",
      "[step: 230] loss: 3.376845598220825\n",
      "[step: 231] loss: 3.36812424659729\n",
      "[step: 232] loss: 3.359494924545288\n",
      "[step: 233] loss: 3.350954532623291\n",
      "[step: 234] loss: 3.342499017715454\n",
      "[step: 235] loss: 3.3341355323791504\n",
      "[step: 236] loss: 3.3258562088012695\n",
      "[step: 237] loss: 3.317659854888916\n",
      "[step: 238] loss: 3.3095459938049316\n",
      "[step: 239] loss: 3.3015120029449463\n",
      "[step: 240] loss: 3.293564796447754\n",
      "[step: 241] loss: 3.2856855392456055\n",
      "[step: 242] loss: 3.2778890132904053\n",
      "[step: 243] loss: 3.270165205001831\n",
      "[step: 244] loss: 3.2625155448913574\n",
      "[step: 245] loss: 3.254941701889038\n",
      "[step: 246] loss: 3.247439384460449\n",
      "[step: 247] loss: 3.240003824234009\n",
      "[step: 248] loss: 3.2326436042785645\n",
      "[step: 249] loss: 3.2253458499908447\n",
      "[step: 250] loss: 3.218116044998169\n",
      "[step: 251] loss: 3.210953950881958\n",
      "[step: 252] loss: 3.2038583755493164\n",
      "[step: 253] loss: 3.1968226432800293\n",
      "[step: 254] loss: 3.189849376678467\n",
      "[step: 255] loss: 3.182943105697632\n",
      "[step: 256] loss: 3.176089286804199\n",
      "[step: 257] loss: 3.169302225112915\n",
      "[step: 258] loss: 3.162569284439087\n",
      "[step: 259] loss: 3.155892848968506\n",
      "[step: 260] loss: 3.1492762565612793\n",
      "[step: 261] loss: 3.1427128314971924\n",
      "[step: 262] loss: 3.136204719543457\n",
      "[step: 263] loss: 3.129751205444336\n",
      "[step: 264] loss: 3.123349905014038\n",
      "[step: 265] loss: 3.116997718811035\n",
      "[step: 266] loss: 3.1106982231140137\n",
      "[step: 267] loss: 3.104451894760132\n",
      "[step: 268] loss: 3.0982518196105957\n",
      "[step: 269] loss: 3.0921008586883545\n",
      "[step: 270] loss: 3.085996150970459\n",
      "[step: 271] loss: 3.079936981201172\n",
      "[step: 272] loss: 3.07392954826355\n",
      "[step: 273] loss: 3.0679638385772705\n",
      "[step: 274] loss: 3.062042236328125\n",
      "[step: 275] loss: 3.0561647415161133\n",
      "[step: 276] loss: 3.0503311157226562\n",
      "[step: 277] loss: 3.044538974761963\n",
      "[step: 278] loss: 3.038790225982666\n",
      "[step: 279] loss: 3.033080577850342\n",
      "[step: 280] loss: 3.027412176132202\n",
      "[step: 281] loss: 3.0217816829681396\n",
      "[step: 282] loss: 3.0161914825439453\n",
      "[step: 283] loss: 3.0106396675109863\n",
      "[step: 284] loss: 3.005124568939209\n",
      "[step: 285] loss: 2.9996461868286133\n",
      "[step: 286] loss: 2.994206666946411\n",
      "[step: 287] loss: 2.9887990951538086\n",
      "[step: 288] loss: 2.983428955078125\n",
      "[step: 289] loss: 2.978092670440674\n",
      "[step: 290] loss: 2.9727916717529297\n",
      "[step: 291] loss: 2.9675211906433105\n",
      "[step: 292] loss: 2.962287425994873\n",
      "[step: 293] loss: 2.9570860862731934\n",
      "[step: 294] loss: 2.9519131183624268\n",
      "[step: 295] loss: 2.9467737674713135\n",
      "[step: 296] loss: 2.9416632652282715\n",
      "[step: 297] loss: 2.9365854263305664\n",
      "[step: 298] loss: 2.9315361976623535\n",
      "[step: 299] loss: 2.926513195037842\n",
      "[step: 300] loss: 2.9215269088745117\n",
      "[step: 301] loss: 2.916563034057617\n",
      "[step: 302] loss: 2.9116299152374268\n",
      "[step: 303] loss: 2.9067206382751465\n",
      "[step: 304] loss: 2.901839256286621\n",
      "[step: 305] loss: 2.8969860076904297\n",
      "[step: 306] loss: 2.892160654067993\n",
      "[step: 307] loss: 2.8873562812805176\n",
      "[step: 308] loss: 2.8825817108154297\n",
      "[step: 309] loss: 2.8778278827667236\n",
      "[step: 310] loss: 2.8731021881103516\n",
      "[step: 311] loss: 2.868398427963257\n",
      "[step: 312] loss: 2.8637187480926514\n",
      "[step: 313] loss: 2.859063148498535\n",
      "[step: 314] loss: 2.854426860809326\n",
      "[step: 315] loss: 2.8498175144195557\n",
      "[step: 316] loss: 2.845229387283325\n",
      "[step: 317] loss: 2.8406617641448975\n",
      "[step: 318] loss: 2.836116313934326\n",
      "[step: 319] loss: 2.8315927982330322\n",
      "[step: 320] loss: 2.827089309692383\n",
      "[step: 321] loss: 2.8226072788238525\n",
      "[step: 322] loss: 2.8181445598602295\n",
      "[step: 323] loss: 2.8137009143829346\n",
      "[step: 324] loss: 2.809278964996338\n",
      "[step: 325] loss: 2.8048739433288574\n",
      "[step: 326] loss: 2.800490617752075\n",
      "[step: 327] loss: 2.7961232662200928\n",
      "[step: 328] loss: 2.791775703430176\n",
      "[step: 329] loss: 2.7874486446380615\n",
      "[step: 330] loss: 2.783134937286377\n",
      "[step: 331] loss: 2.778839588165283\n",
      "[step: 332] loss: 2.774562358856201\n",
      "[step: 333] loss: 2.7703046798706055\n",
      "[step: 334] loss: 2.7660610675811768\n",
      "[step: 335] loss: 2.7618370056152344\n",
      "[step: 336] loss: 2.75762677192688\n",
      "[step: 337] loss: 2.753431797027588\n",
      "[step: 338] loss: 2.749253988265991\n",
      "[step: 339] loss: 2.7450921535491943\n",
      "[step: 340] loss: 2.7409448623657227\n",
      "[step: 341] loss: 2.7368111610412598\n",
      "[step: 342] loss: 2.732696056365967\n",
      "[step: 343] loss: 2.7285940647125244\n",
      "[step: 344] loss: 2.7245073318481445\n",
      "[step: 345] loss: 2.7204341888427734\n",
      "[step: 346] loss: 2.7163748741149902\n",
      "[step: 347] loss: 2.7123284339904785\n",
      "[step: 348] loss: 2.708296298980713\n",
      "[step: 349] loss: 2.7042784690856934\n",
      "[step: 350] loss: 2.7002742290496826\n",
      "[step: 351] loss: 2.696284532546997\n",
      "[step: 352] loss: 2.6923060417175293\n",
      "[step: 353] loss: 2.6883418560028076\n",
      "[step: 354] loss: 2.6843886375427246\n",
      "[step: 355] loss: 2.680449962615967\n",
      "[step: 356] loss: 2.6765193939208984\n",
      "[step: 357] loss: 2.6726038455963135\n",
      "[step: 358] loss: 2.6687018871307373\n",
      "[step: 359] loss: 2.6648123264312744\n",
      "[step: 360] loss: 2.6609320640563965\n",
      "[step: 361] loss: 2.657066583633423\n",
      "[step: 362] loss: 2.6532092094421387\n",
      "[step: 363] loss: 2.6493635177612305\n",
      "[step: 364] loss: 2.6455295085906982\n",
      "[step: 365] loss: 2.6417062282562256\n",
      "[step: 366] loss: 2.6378962993621826\n",
      "[step: 367] loss: 2.634091377258301\n",
      "[step: 368] loss: 2.6303014755249023\n",
      "[step: 369] loss: 2.626523494720459\n",
      "[step: 370] loss: 2.622756004333496\n",
      "[step: 371] loss: 2.618997573852539\n",
      "[step: 372] loss: 2.6152498722076416\n",
      "[step: 373] loss: 2.611511468887329\n",
      "[step: 374] loss: 2.607783794403076\n",
      "[step: 375] loss: 2.6040658950805664\n",
      "[step: 376] loss: 2.6003572940826416\n",
      "[step: 377] loss: 2.596656560897827\n",
      "[step: 378] loss: 2.592968225479126\n",
      "[step: 379] loss: 2.589287757873535\n",
      "[step: 380] loss: 2.5856189727783203\n",
      "[step: 381] loss: 2.581958293914795\n",
      "[step: 382] loss: 2.5783088207244873\n",
      "[step: 383] loss: 2.574665069580078\n",
      "[step: 384] loss: 2.5710318088531494\n",
      "[step: 385] loss: 2.5674076080322266\n",
      "[step: 386] loss: 2.5637941360473633\n",
      "[step: 387] loss: 2.560185670852661\n",
      "[step: 388] loss: 2.556589126586914\n",
      "[step: 389] loss: 2.552999973297119\n",
      "[step: 390] loss: 2.549419403076172\n",
      "[step: 391] loss: 2.5458455085754395\n",
      "[step: 392] loss: 2.5422844886779785\n",
      "[step: 393] loss: 2.5387284755706787\n",
      "[step: 394] loss: 2.535182476043701\n",
      "[step: 395] loss: 2.531643867492676\n",
      "[step: 396] loss: 2.5281152725219727\n",
      "[step: 397] loss: 2.5245912075042725\n",
      "[step: 398] loss: 2.52107834815979\n",
      "[step: 399] loss: 2.5175728797912598\n",
      "[step: 400] loss: 2.514075756072998\n",
      "[step: 401] loss: 2.51058292388916\n",
      "[step: 402] loss: 2.507103443145752\n",
      "[step: 403] loss: 2.5036253929138184\n",
      "[step: 404] loss: 2.5001564025878906\n",
      "[step: 405] loss: 2.496699333190918\n",
      "[step: 406] loss: 2.4932503700256348\n",
      "[step: 407] loss: 2.489804267883301\n",
      "[step: 408] loss: 2.4863686561584473\n",
      "[step: 409] loss: 2.482940196990967\n",
      "[step: 410] loss: 2.479515314102173\n",
      "[step: 411] loss: 2.476102352142334\n",
      "[step: 412] loss: 2.4726955890655518\n",
      "[step: 413] loss: 2.469296455383301\n",
      "[step: 414] loss: 2.4659054279327393\n",
      "[step: 415] loss: 2.4625167846679688\n",
      "[step: 416] loss: 2.4591403007507324\n",
      "[step: 417] loss: 2.4557673931121826\n",
      "[step: 418] loss: 2.4524028301239014\n",
      "[step: 419] loss: 2.4490466117858887\n",
      "[step: 420] loss: 2.4456958770751953\n",
      "[step: 421] loss: 2.442352056503296\n",
      "[step: 422] loss: 2.4390177726745605\n",
      "[step: 423] loss: 2.4356865882873535\n",
      "[step: 424] loss: 2.432366132736206\n",
      "[step: 425] loss: 2.429048776626587\n",
      "[step: 426] loss: 2.4257402420043945\n",
      "[step: 427] loss: 2.4224369525909424\n",
      "[step: 428] loss: 2.419142723083496\n",
      "[step: 429] loss: 2.415851593017578\n",
      "[step: 430] loss: 2.412569761276245\n",
      "[step: 431] loss: 2.4092941284179688\n",
      "[step: 432] loss: 2.406024932861328\n",
      "[step: 433] loss: 2.4027621746063232\n",
      "[step: 434] loss: 2.399506092071533\n",
      "[step: 435] loss: 2.3962552547454834\n",
      "[step: 436] loss: 2.3930134773254395\n",
      "[step: 437] loss: 2.3897767066955566\n",
      "[step: 438] loss: 2.386544704437256\n",
      "[step: 439] loss: 2.383322238922119\n",
      "[step: 440] loss: 2.3801069259643555\n",
      "[step: 441] loss: 2.376896381378174\n",
      "[step: 442] loss: 2.3736894130706787\n",
      "[step: 443] loss: 2.3704915046691895\n",
      "[step: 444] loss: 2.367300033569336\n",
      "[step: 445] loss: 2.364114761352539\n",
      "[step: 446] loss: 2.3609352111816406\n",
      "[step: 447] loss: 2.3577609062194824\n",
      "[step: 448] loss: 2.354593276977539\n",
      "[step: 449] loss: 2.351433038711548\n",
      "[step: 450] loss: 2.3482789993286133\n",
      "[step: 451] loss: 2.3451294898986816\n",
      "[step: 452] loss: 2.341987133026123\n",
      "[step: 453] loss: 2.3388519287109375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 454] loss: 2.335723638534546\n",
      "[step: 455] loss: 2.3325986862182617\n",
      "[step: 456] loss: 2.3294782638549805\n",
      "[step: 457] loss: 2.3263680934906006\n",
      "[step: 458] loss: 2.3232619762420654\n",
      "[step: 459] loss: 2.3201615810394287\n",
      "[step: 460] loss: 2.3170671463012695\n",
      "[step: 461] loss: 2.3139829635620117\n",
      "[step: 462] loss: 2.310903787612915\n",
      "[step: 463] loss: 2.3078255653381348\n",
      "[step: 464] loss: 2.3047537803649902\n",
      "[step: 465] loss: 2.301692008972168\n",
      "[step: 466] loss: 2.2986340522766113\n",
      "[step: 467] loss: 2.295583486557007\n",
      "[step: 468] loss: 2.292539596557617\n",
      "[step: 469] loss: 2.2894976139068604\n",
      "[step: 470] loss: 2.286466121673584\n",
      "[step: 471] loss: 2.2834393978118896\n",
      "[step: 472] loss: 2.2804155349731445\n",
      "[step: 473] loss: 2.2773995399475098\n",
      "[step: 474] loss: 2.2743916511535645\n",
      "[step: 475] loss: 2.271385669708252\n",
      "[step: 476] loss: 2.2683887481689453\n",
      "[step: 477] loss: 2.265397071838379\n",
      "[step: 478] loss: 2.2624104022979736\n",
      "[step: 479] loss: 2.259431838989258\n",
      "[step: 480] loss: 2.2564563751220703\n",
      "[step: 481] loss: 2.253488063812256\n",
      "[step: 482] loss: 2.2505266666412354\n",
      "[step: 483] loss: 2.2475709915161133\n",
      "[step: 484] loss: 2.244621753692627\n",
      "[step: 485] loss: 2.2416772842407227\n",
      "[step: 486] loss: 2.2387380599975586\n",
      "[step: 487] loss: 2.2358062267303467\n",
      "[step: 488] loss: 2.2328805923461914\n",
      "[step: 489] loss: 2.2299585342407227\n",
      "[step: 490] loss: 2.227043867111206\n",
      "[step: 491] loss: 2.224135637283325\n",
      "[step: 492] loss: 2.2212324142456055\n",
      "[step: 493] loss: 2.218334197998047\n",
      "[step: 494] loss: 2.2154436111450195\n",
      "[step: 495] loss: 2.212559223175049\n",
      "[step: 496] loss: 2.2096786499023438\n",
      "[step: 497] loss: 2.2068095207214355\n",
      "[step: 498] loss: 2.203939914703369\n",
      "[step: 499] loss: 2.201078414916992\n",
      "[step: 500] loss: 2.1982243061065674\n",
      "[step: 501] loss: 2.195374011993408\n",
      "[step: 502] loss: 2.192531108856201\n",
      "[step: 503] loss: 2.1896939277648926\n",
      "[step: 504] loss: 2.1868605613708496\n",
      "[step: 505] loss: 2.1840386390686035\n",
      "[step: 506] loss: 2.1812174320220947\n",
      "[step: 507] loss: 2.17840576171875\n",
      "[step: 508] loss: 2.175597906112671\n",
      "[step: 509] loss: 2.172797679901123\n",
      "[step: 510] loss: 2.1700029373168945\n",
      "[step: 511] loss: 2.1672139167785645\n",
      "[step: 512] loss: 2.1644320487976074\n",
      "[step: 513] loss: 2.161653518676758\n",
      "[step: 514] loss: 2.158884048461914\n",
      "[step: 515] loss: 2.1561179161071777\n",
      "[step: 516] loss: 2.1533613204956055\n",
      "[step: 517] loss: 2.1506075859069824\n",
      "[step: 518] loss: 2.147861957550049\n",
      "[step: 519] loss: 2.145123243331909\n",
      "[step: 520] loss: 2.142388343811035\n",
      "[step: 521] loss: 2.1396608352661133\n",
      "[step: 522] loss: 2.136939287185669\n",
      "[step: 523] loss: 2.1342239379882812\n",
      "[step: 524] loss: 2.1315150260925293\n",
      "[step: 525] loss: 2.1288137435913086\n",
      "[step: 526] loss: 2.126114845275879\n",
      "[step: 527] loss: 2.123427152633667\n",
      "[step: 528] loss: 2.120744228363037\n",
      "[step: 529] loss: 2.118065357208252\n",
      "[step: 530] loss: 2.11539363861084\n",
      "[step: 531] loss: 2.1127307415008545\n",
      "[step: 532] loss: 2.110069990158081\n",
      "[step: 533] loss: 2.107417583465576\n",
      "[step: 534] loss: 2.1047730445861816\n",
      "[step: 535] loss: 2.102133274078369\n",
      "[step: 536] loss: 2.099500894546509\n",
      "[step: 537] loss: 2.096872568130493\n",
      "[step: 538] loss: 2.0942554473876953\n",
      "[step: 539] loss: 2.0916402339935303\n",
      "[step: 540] loss: 2.0890328884124756\n",
      "[step: 541] loss: 2.0864341259002686\n",
      "[step: 542] loss: 2.0838401317596436\n",
      "[step: 543] loss: 2.081251382827759\n",
      "[step: 544] loss: 2.0786709785461426\n",
      "[step: 545] loss: 2.076097011566162\n",
      "[step: 546] loss: 2.0735292434692383\n",
      "[step: 547] loss: 2.0709667205810547\n",
      "[step: 548] loss: 2.0684123039245605\n",
      "[step: 549] loss: 2.0658669471740723\n",
      "[step: 550] loss: 2.063324213027954\n",
      "[step: 551] loss: 2.0607919692993164\n",
      "[step: 552] loss: 2.0582618713378906\n",
      "[step: 553] loss: 2.0557403564453125\n",
      "[step: 554] loss: 2.0532279014587402\n",
      "[step: 555] loss: 2.050718307495117\n",
      "[step: 556] loss: 2.048220157623291\n",
      "[step: 557] loss: 2.045726776123047\n",
      "[step: 558] loss: 2.043238401412964\n",
      "[step: 559] loss: 2.0407581329345703\n",
      "[step: 560] loss: 2.03828763961792\n",
      "[step: 561] loss: 2.035818099975586\n",
      "[step: 562] loss: 2.033360004425049\n",
      "[step: 563] loss: 2.0309062004089355\n",
      "[step: 564] loss: 2.0284600257873535\n",
      "[step: 565] loss: 2.0260210037231445\n",
      "[step: 566] loss: 2.023590326309204\n",
      "[step: 567] loss: 2.0211658477783203\n",
      "[step: 568] loss: 2.0187478065490723\n",
      "[step: 569] loss: 2.0163354873657227\n",
      "[step: 570] loss: 2.0139319896698\n",
      "[step: 571] loss: 2.0115370750427246\n",
      "[step: 572] loss: 2.0091464519500732\n",
      "[step: 573] loss: 2.006763458251953\n",
      "[step: 574] loss: 2.004387855529785\n",
      "[step: 575] loss: 2.0020201206207275\n",
      "[step: 576] loss: 1.9996583461761475\n",
      "[step: 577] loss: 1.997304081916809\n",
      "[step: 578] loss: 1.9949579238891602\n",
      "[step: 579] loss: 1.9926176071166992\n",
      "[step: 580] loss: 1.9902846813201904\n",
      "[step: 581] loss: 1.9879592657089233\n",
      "[step: 582] loss: 1.9856406450271606\n",
      "[step: 583] loss: 1.9833297729492188\n",
      "[step: 584] loss: 1.9810256958007812\n",
      "[step: 585] loss: 1.9787297248840332\n",
      "[step: 586] loss: 1.9764400720596313\n",
      "[step: 587] loss: 1.9741578102111816\n",
      "[step: 588] loss: 1.9718841314315796\n",
      "[step: 589] loss: 1.9696152210235596\n",
      "[step: 590] loss: 1.9673545360565186\n",
      "[step: 591] loss: 1.9651025533676147\n",
      "[step: 592] loss: 1.9628559350967407\n",
      "[step: 593] loss: 1.9606168270111084\n",
      "[step: 594] loss: 1.9583854675292969\n",
      "[step: 595] loss: 1.9561625719070435\n",
      "[step: 596] loss: 1.9539462327957153\n",
      "[step: 597] loss: 1.951737880706787\n",
      "[step: 598] loss: 1.9495337009429932\n",
      "[step: 599] loss: 1.9473395347595215\n",
      "[step: 600] loss: 1.9451541900634766\n",
      "[step: 601] loss: 1.9429750442504883\n",
      "[step: 602] loss: 1.9408009052276611\n",
      "[step: 603] loss: 1.9386372566223145\n",
      "[step: 604] loss: 1.9364789724349976\n",
      "[step: 605] loss: 1.9343279600143433\n",
      "[step: 606] loss: 1.9321843385696411\n",
      "[step: 607] loss: 1.930047631263733\n",
      "[step: 608] loss: 1.9279201030731201\n",
      "[step: 609] loss: 1.9257988929748535\n",
      "[step: 610] loss: 1.9236853122711182\n",
      "[step: 611] loss: 1.9215784072875977\n",
      "[step: 612] loss: 1.919477939605713\n",
      "[step: 613] loss: 1.9173872470855713\n",
      "[step: 614] loss: 1.9153037071228027\n",
      "[step: 615] loss: 1.9132251739501953\n",
      "[step: 616] loss: 1.9111543893814087\n",
      "[step: 617] loss: 1.9090919494628906\n",
      "[step: 618] loss: 1.9070364236831665\n",
      "[step: 619] loss: 1.9049878120422363\n",
      "[step: 620] loss: 1.902948021888733\n",
      "[step: 621] loss: 1.9009130001068115\n",
      "[step: 622] loss: 1.898885726928711\n",
      "[step: 623] loss: 1.8968653678894043\n",
      "[step: 624] loss: 1.8948547840118408\n",
      "[step: 625] loss: 1.892849087715149\n",
      "[step: 626] loss: 1.8908514976501465\n",
      "[step: 627] loss: 1.8888604640960693\n",
      "[step: 628] loss: 1.8868759870529175\n",
      "[step: 629] loss: 1.8848994970321655\n",
      "[step: 630] loss: 1.8829303979873657\n",
      "[step: 631] loss: 1.8809679746627808\n",
      "[step: 632] loss: 1.8790135383605957\n",
      "[step: 633] loss: 1.8770651817321777\n",
      "[step: 634] loss: 1.8751232624053955\n",
      "[step: 635] loss: 1.8731892108917236\n",
      "[step: 636] loss: 1.8712620735168457\n",
      "[step: 637] loss: 1.8693411350250244\n",
      "[step: 638] loss: 1.8674283027648926\n",
      "[step: 639] loss: 1.8655204772949219\n",
      "[step: 640] loss: 1.8636205196380615\n",
      "[step: 641] loss: 1.861728310585022\n",
      "[step: 642] loss: 1.859842300415039\n",
      "[step: 643] loss: 1.8579634428024292\n",
      "[step: 644] loss: 1.8560909032821655\n",
      "[step: 645] loss: 1.8542243242263794\n",
      "[step: 646] loss: 1.8523659706115723\n",
      "[step: 647] loss: 1.8505133390426636\n",
      "[step: 648] loss: 1.8486679792404175\n",
      "[step: 649] loss: 1.846827507019043\n",
      "[step: 650] loss: 1.8449947834014893\n",
      "[step: 651] loss: 1.8431689739227295\n",
      "[step: 652] loss: 1.8413505554199219\n",
      "[step: 653] loss: 1.8395376205444336\n",
      "[step: 654] loss: 1.8377324342727661\n",
      "[step: 655] loss: 1.8359315395355225\n",
      "[step: 656] loss: 1.8341383934020996\n",
      "[step: 657] loss: 1.8323519229888916\n",
      "[step: 658] loss: 1.8305704593658447\n",
      "[step: 659] loss: 1.8287959098815918\n",
      "[step: 660] loss: 1.8270292282104492\n",
      "[step: 661] loss: 1.8252671957015991\n",
      "[step: 662] loss: 1.8235114812850952\n",
      "[step: 663] loss: 1.8217618465423584\n",
      "[step: 664] loss: 1.8200186491012573\n",
      "[step: 665] loss: 1.8182820081710815\n",
      "[step: 666] loss: 1.8165497779846191\n",
      "[step: 667] loss: 1.8148267269134521\n",
      "[step: 668] loss: 1.8131070137023926\n",
      "[step: 669] loss: 1.811395525932312\n",
      "[step: 670] loss: 1.8096877336502075\n",
      "[step: 671] loss: 1.8079876899719238\n",
      "[step: 672] loss: 1.8062913417816162\n",
      "[step: 673] loss: 1.804602861404419\n",
      "[step: 674] loss: 1.8029203414916992\n",
      "[step: 675] loss: 1.8012425899505615\n",
      "[step: 676] loss: 1.7995713949203491\n",
      "[step: 677] loss: 1.79790461063385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 678] loss: 1.7962454557418823\n",
      "[step: 679] loss: 1.7945902347564697\n",
      "[step: 680] loss: 1.7929418087005615\n",
      "[step: 681] loss: 1.7912983894348145\n",
      "[step: 682] loss: 1.7896608114242554\n",
      "[step: 683] loss: 1.7880282402038574\n",
      "[step: 684] loss: 1.7864012718200684\n",
      "[step: 685] loss: 1.7847820520401\n",
      "[step: 686] loss: 1.7831648588180542\n",
      "[step: 687] loss: 1.7815552949905396\n",
      "[step: 688] loss: 1.779950499534607\n",
      "[step: 689] loss: 1.7783503532409668\n",
      "[step: 690] loss: 1.7767560482025146\n",
      "[step: 691] loss: 1.7751667499542236\n",
      "[step: 692] loss: 1.7735824584960938\n",
      "[step: 693] loss: 1.7720038890838623\n",
      "[step: 694] loss: 1.7704309225082397\n",
      "[step: 695] loss: 1.7688629627227783\n",
      "[step: 696] loss: 1.7672994136810303\n",
      "[step: 697] loss: 1.7657413482666016\n",
      "[step: 698] loss: 1.7641876935958862\n",
      "[step: 699] loss: 1.7626394033432007\n",
      "[step: 700] loss: 1.7610973119735718\n",
      "[step: 701] loss: 1.7595574855804443\n",
      "[step: 702] loss: 1.7580242156982422\n",
      "[step: 703] loss: 1.7564966678619385\n",
      "[step: 704] loss: 1.7549725770950317\n",
      "[step: 705] loss: 1.7534538507461548\n",
      "[step: 706] loss: 1.7519404888153076\n",
      "[step: 707] loss: 1.7504303455352783\n",
      "[step: 708] loss: 1.7489243745803833\n",
      "[step: 709] loss: 1.74742591381073\n",
      "[step: 710] loss: 1.7459299564361572\n",
      "[step: 711] loss: 1.7444390058517456\n",
      "[step: 712] loss: 1.7429533004760742\n",
      "[step: 713] loss: 1.7414724826812744\n",
      "[step: 714] loss: 1.7399951219558716\n",
      "[step: 715] loss: 1.7385227680206299\n",
      "[step: 716] loss: 1.7370553016662598\n",
      "[step: 717] loss: 1.7355902194976807\n",
      "[step: 718] loss: 1.7341322898864746\n",
      "[step: 719] loss: 1.7326772212982178\n",
      "[step: 720] loss: 1.731227159500122\n",
      "[step: 721] loss: 1.729780912399292\n",
      "[step: 722] loss: 1.728338360786438\n",
      "[step: 723] loss: 1.7269024848937988\n",
      "[step: 724] loss: 1.7254695892333984\n",
      "[step: 725] loss: 1.724039912223816\n",
      "[step: 726] loss: 1.72261381149292\n",
      "[step: 727] loss: 1.721196174621582\n",
      "[step: 728] loss: 1.7197792530059814\n",
      "[step: 729] loss: 1.7183635234832764\n",
      "[step: 730] loss: 1.7169568538665771\n",
      "[step: 731] loss: 1.7155532836914062\n",
      "[step: 732] loss: 1.7141528129577637\n",
      "[step: 733] loss: 1.7127575874328613\n",
      "[step: 734] loss: 1.7113661766052246\n",
      "[step: 735] loss: 1.7099765539169312\n",
      "[step: 736] loss: 1.7085932493209839\n",
      "[step: 737] loss: 1.7072131633758545\n",
      "[step: 738] loss: 1.705836534500122\n",
      "[step: 739] loss: 1.7044649124145508\n",
      "[step: 740] loss: 1.7030956745147705\n",
      "[step: 741] loss: 1.7017316818237305\n",
      "[step: 742] loss: 1.7003684043884277\n",
      "[step: 743] loss: 1.6990119218826294\n",
      "[step: 744] loss: 1.6976585388183594\n",
      "[step: 745] loss: 1.6963083744049072\n",
      "[step: 746] loss: 1.6949632167816162\n",
      "[step: 747] loss: 1.6936194896697998\n",
      "[step: 748] loss: 1.6922805309295654\n",
      "[step: 749] loss: 1.6909462213516235\n",
      "[step: 750] loss: 1.6896145343780518\n",
      "[step: 751] loss: 1.688286304473877\n",
      "[step: 752] loss: 1.6869618892669678\n",
      "[step: 753] loss: 1.6856415271759033\n",
      "[step: 754] loss: 1.6843247413635254\n",
      "[step: 755] loss: 1.6830098628997803\n",
      "[step: 756] loss: 1.6817009449005127\n",
      "[step: 757] loss: 1.6803933382034302\n",
      "[step: 758] loss: 1.6790904998779297\n",
      "[step: 759] loss: 1.6777904033660889\n",
      "[step: 760] loss: 1.6764941215515137\n",
      "[step: 761] loss: 1.6751996278762817\n",
      "[step: 762] loss: 1.6739106178283691\n",
      "[step: 763] loss: 1.6726243495941162\n",
      "[step: 764] loss: 1.6713420152664185\n",
      "[step: 765] loss: 1.6700618267059326\n",
      "[step: 766] loss: 1.6687850952148438\n",
      "[step: 767] loss: 1.6675108671188354\n",
      "[step: 768] loss: 1.6662414073944092\n",
      "[step: 769] loss: 1.6649763584136963\n",
      "[step: 770] loss: 1.6637122631072998\n",
      "[step: 771] loss: 1.6624524593353271\n",
      "[step: 772] loss: 1.6611942052841187\n",
      "[step: 773] loss: 1.6599411964416504\n",
      "[step: 774] loss: 1.6586898565292358\n",
      "[step: 775] loss: 1.6574430465698242\n",
      "[step: 776] loss: 1.6561989784240723\n",
      "[step: 777] loss: 1.6549564599990845\n",
      "[step: 778] loss: 1.6537184715270996\n",
      "[step: 779] loss: 1.6524829864501953\n",
      "[step: 780] loss: 1.6512507200241089\n",
      "[step: 781] loss: 1.650022268295288\n",
      "[step: 782] loss: 1.6487956047058105\n",
      "[step: 783] loss: 1.647573471069336\n",
      "[step: 784] loss: 1.6463525295257568\n",
      "[step: 785] loss: 1.6451348066329956\n",
      "[step: 786] loss: 1.6439204216003418\n",
      "[step: 787] loss: 1.6427106857299805\n",
      "[step: 788] loss: 1.6415002346038818\n",
      "[step: 789] loss: 1.6402952671051025\n",
      "[step: 790] loss: 1.6390936374664307\n",
      "[step: 791] loss: 1.6378949880599976\n",
      "[step: 792] loss: 1.636697769165039\n",
      "[step: 793] loss: 1.6355022192001343\n",
      "[step: 794] loss: 1.6343131065368652\n",
      "[step: 795] loss: 1.6331243515014648\n",
      "[step: 796] loss: 1.6319379806518555\n",
      "[step: 797] loss: 1.6307556629180908\n",
      "[step: 798] loss: 1.6295766830444336\n",
      "[step: 799] loss: 1.6283986568450928\n",
      "[step: 800] loss: 1.6272242069244385\n",
      "[step: 801] loss: 1.6260530948638916\n",
      "[step: 802] loss: 1.6248841285705566\n",
      "[step: 803] loss: 1.6237163543701172\n",
      "[step: 804] loss: 1.6225526332855225\n",
      "[step: 805] loss: 1.6213939189910889\n",
      "[step: 806] loss: 1.6202359199523926\n",
      "[step: 807] loss: 1.61907958984375\n",
      "[step: 808] loss: 1.6179265975952148\n",
      "[step: 809] loss: 1.616776704788208\n",
      "[step: 810] loss: 1.615628719329834\n",
      "[step: 811] loss: 1.6144850254058838\n",
      "[step: 812] loss: 1.6133416891098022\n",
      "[step: 813] loss: 1.6122026443481445\n",
      "[step: 814] loss: 1.6110668182373047\n",
      "[step: 815] loss: 1.609930157661438\n",
      "[step: 816] loss: 1.6087994575500488\n",
      "[step: 817] loss: 1.6076698303222656\n",
      "[step: 818] loss: 1.6065430641174316\n",
      "[step: 819] loss: 1.605418086051941\n",
      "[step: 820] loss: 1.604296088218689\n",
      "[step: 821] loss: 1.6031781435012817\n",
      "[step: 822] loss: 1.6020594835281372\n",
      "[step: 823] loss: 1.6009464263916016\n",
      "[step: 824] loss: 1.599834680557251\n",
      "[step: 825] loss: 1.598724126815796\n",
      "[step: 826] loss: 1.597617506980896\n",
      "[step: 827] loss: 1.5965124368667603\n",
      "[step: 828] loss: 1.5954105854034424\n",
      "[step: 829] loss: 1.5943108797073364\n",
      "[step: 830] loss: 1.5932135581970215\n",
      "[step: 831] loss: 1.5921200513839722\n",
      "[step: 832] loss: 1.5910260677337646\n",
      "[step: 833] loss: 1.5899356603622437\n",
      "[step: 834] loss: 1.5888488292694092\n",
      "[step: 835] loss: 1.5877631902694702\n",
      "[step: 836] loss: 1.5866793394088745\n",
      "[step: 837] loss: 1.5855985879898071\n",
      "[step: 838] loss: 1.5845203399658203\n",
      "[step: 839] loss: 1.5834441184997559\n",
      "[step: 840] loss: 1.5823702812194824\n",
      "[step: 841] loss: 1.5812993049621582\n",
      "[step: 842] loss: 1.58022940158844\n",
      "[step: 843] loss: 1.57916259765625\n",
      "[step: 844] loss: 1.5780985355377197\n",
      "[step: 845] loss: 1.577035903930664\n",
      "[step: 846] loss: 1.5759764909744263\n",
      "[step: 847] loss: 1.5749188661575317\n",
      "[step: 848] loss: 1.5738613605499268\n",
      "[step: 849] loss: 1.5728096961975098\n",
      "[step: 850] loss: 1.57175874710083\n",
      "[step: 851] loss: 1.5707095861434937\n",
      "[step: 852] loss: 1.5696628093719482\n",
      "[step: 853] loss: 1.5686185359954834\n",
      "[step: 854] loss: 1.567576289176941\n",
      "[step: 855] loss: 1.5665349960327148\n",
      "[step: 856] loss: 1.5654971599578857\n",
      "[step: 857] loss: 1.5644618272781372\n",
      "[step: 858] loss: 1.5634288787841797\n",
      "[step: 859] loss: 1.5623979568481445\n",
      "[step: 860] loss: 1.5613679885864258\n",
      "[step: 861] loss: 1.5603405237197876\n",
      "[step: 862] loss: 1.5593162775039673\n",
      "[step: 863] loss: 1.5582921504974365\n",
      "[step: 864] loss: 1.5572712421417236\n",
      "[step: 865] loss: 1.5562548637390137\n",
      "[step: 866] loss: 1.5552361011505127\n",
      "[step: 867] loss: 1.554222822189331\n",
      "[step: 868] loss: 1.5532095432281494\n",
      "[step: 869] loss: 1.5521998405456543\n",
      "[step: 870] loss: 1.5511912107467651\n",
      "[step: 871] loss: 1.5501850843429565\n",
      "[step: 872] loss: 1.549180269241333\n",
      "[step: 873] loss: 1.5481789112091064\n",
      "[step: 874] loss: 1.5471786260604858\n",
      "[step: 875] loss: 1.5461807250976562\n",
      "[step: 876] loss: 1.5451849699020386\n",
      "[step: 877] loss: 1.5441904067993164\n",
      "[step: 878] loss: 1.5431993007659912\n",
      "[step: 879] loss: 1.5422096252441406\n",
      "[step: 880] loss: 1.5412205457687378\n",
      "[step: 881] loss: 1.5402344465255737\n",
      "[step: 882] loss: 1.5392515659332275\n",
      "[step: 883] loss: 1.5382682085037231\n",
      "[step: 884] loss: 1.5372893810272217\n",
      "[step: 885] loss: 1.536311149597168\n",
      "[step: 886] loss: 1.535334825515747\n",
      "[step: 887] loss: 1.5343612432479858\n",
      "[step: 888] loss: 1.5333892107009888\n",
      "[step: 889] loss: 1.5324199199676514\n",
      "[step: 890] loss: 1.5314521789550781\n",
      "[step: 891] loss: 1.5304856300354004\n",
      "[step: 892] loss: 1.5295220613479614\n",
      "[step: 893] loss: 1.5285587310791016\n",
      "[step: 894] loss: 1.527598261833191\n",
      "[step: 895] loss: 1.5266399383544922\n",
      "[step: 896] loss: 1.5256836414337158\n",
      "[step: 897] loss: 1.5247280597686768\n",
      "[step: 898] loss: 1.5237767696380615\n",
      "[step: 899] loss: 1.5228244066238403\n",
      "[step: 900] loss: 1.5218768119812012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step: 901] loss: 1.5209310054779053\n",
      "[step: 902] loss: 1.5199832916259766\n",
      "[step: 903] loss: 1.519040584564209\n",
      "[step: 904] loss: 1.5180997848510742\n",
      "[step: 905] loss: 1.5171594619750977\n",
      "[step: 906] loss: 1.5162229537963867\n",
      "[step: 907] loss: 1.515285849571228\n",
      "[step: 908] loss: 1.5143535137176514\n",
      "[step: 909] loss: 1.5134197473526\n",
      "[step: 910] loss: 1.5124905109405518\n",
      "[step: 911] loss: 1.5115630626678467\n",
      "[step: 912] loss: 1.5106356143951416\n",
      "[step: 913] loss: 1.5097098350524902\n",
      "[step: 914] loss: 1.5087870359420776\n",
      "[step: 915] loss: 1.507866621017456\n",
      "[step: 916] loss: 1.5069469213485718\n",
      "[step: 917] loss: 1.5060300827026367\n",
      "[step: 918] loss: 1.5051134824752808\n",
      "[step: 919] loss: 1.5042002201080322\n",
      "[step: 920] loss: 1.503287672996521\n",
      "[step: 921] loss: 1.502377986907959\n",
      "[step: 922] loss: 1.5014703273773193\n",
      "[step: 923] loss: 1.500562071800232\n",
      "[step: 924] loss: 1.4996575117111206\n",
      "[step: 925] loss: 1.4987540245056152\n",
      "[step: 926] loss: 1.4978522062301636\n",
      "[step: 927] loss: 1.4969515800476074\n",
      "[step: 928] loss: 1.4960551261901855\n",
      "[step: 929] loss: 1.4951591491699219\n",
      "[step: 930] loss: 1.4942649602890015\n",
      "[step: 931] loss: 1.4933717250823975\n",
      "[step: 932] loss: 1.4924819469451904\n",
      "[step: 933] loss: 1.4915924072265625\n",
      "[step: 934] loss: 1.4907054901123047\n",
      "[step: 935] loss: 1.4898197650909424\n",
      "[step: 936] loss: 1.4889358282089233\n",
      "[step: 937] loss: 1.4880529642105103\n",
      "[step: 938] loss: 1.487173080444336\n",
      "[step: 939] loss: 1.486295223236084\n",
      "[step: 940] loss: 1.4854180812835693\n",
      "[step: 941] loss: 1.484542965888977\n",
      "[step: 942] loss: 1.4836690425872803\n",
      "[step: 943] loss: 1.482797622680664\n",
      "[step: 944] loss: 1.4819259643554688\n",
      "[step: 945] loss: 1.4810590744018555\n",
      "[step: 946] loss: 1.480191946029663\n",
      "[step: 947] loss: 1.479325532913208\n",
      "[step: 948] loss: 1.4784631729125977\n",
      "[step: 949] loss: 1.477602243423462\n",
      "[step: 950] loss: 1.4767415523529053\n",
      "[step: 951] loss: 1.4758834838867188\n",
      "[step: 952] loss: 1.4750268459320068\n",
      "[step: 953] loss: 1.4741723537445068\n",
      "[step: 954] loss: 1.4733185768127441\n",
      "[step: 955] loss: 1.472467064857483\n",
      "[step: 956] loss: 1.4716160297393799\n",
      "[step: 957] loss: 1.4707683324813843\n",
      "[step: 958] loss: 1.4699219465255737\n",
      "[step: 959] loss: 1.46907639503479\n",
      "[step: 960] loss: 1.4682339429855347\n",
      "[step: 961] loss: 1.467392086982727\n",
      "[step: 962] loss: 1.4665521383285522\n",
      "[step: 963] loss: 1.4657127857208252\n",
      "[step: 964] loss: 1.4648768901824951\n",
      "[step: 965] loss: 1.4640417098999023\n",
      "[step: 966] loss: 1.4632078409194946\n",
      "[step: 967] loss: 1.4623758792877197\n",
      "[step: 968] loss: 1.461546540260315\n",
      "[step: 969] loss: 1.4607185125350952\n",
      "[step: 970] loss: 1.45989191532135\n",
      "[step: 971] loss: 1.45906400680542\n",
      "[step: 972] loss: 1.4582411050796509\n",
      "[step: 973] loss: 1.4574201107025146\n",
      "[step: 974] loss: 1.456599473953247\n",
      "[step: 975] loss: 1.4557814598083496\n",
      "[step: 976] loss: 1.4549634456634521\n",
      "[step: 977] loss: 1.4541466236114502\n",
      "[step: 978] loss: 1.4533326625823975\n",
      "[step: 979] loss: 1.4525200128555298\n",
      "[step: 980] loss: 1.4517091512680054\n",
      "[step: 981] loss: 1.4509007930755615\n",
      "[step: 982] loss: 1.450092077255249\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training step\n",
    "    for i in range(iterations):\n",
    "        _, step_loss = sess.run([train, loss], feed_dict={\n",
    "                                X: trainX, Y: trainY})\n",
    "        if i% 200 == 0:\n",
    "            print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
    "\n",
    "    # Test step\n",
    "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
    "    rmse_val = sess.run(rmse, feed_dict={\n",
    "                    targets: testY, predictions: test_predict})\n",
    "    print(\"RMSE: {}\".format(rmse_val))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(testY,label=\"v\")\n",
    "    plt.plot(test_predict,label=\"p\")\n",
    "    plt.xlabel(\"Time Period\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzs3Xd81fXZ+P/XlQ2BJIwQMtl7Q0BA\nwYmiUsRWW62zrbVqbXu3ta1d3q2tv+q3y9q6t/a21i1VFBxVQRAIM4MVIJABSRhZQPb1++NzAodw\ncnJykpN5PR+P80jOZ+U6KLl4z0tUFWOMMcZfQR0dgDHGmK7NEokxxphWsURijDGmVSyRGGOMaRVL\nJMYYY1rFEokxxphWsURijDGmVQKaSERkoYjsEJFsEbnbw/n5IrJRRGpF5KpG51JEZIWIbBORLBEZ\n6jr+nIjsFZHNrtfUQH4GY4wx3oUE6sEiEgw8DCwA8oD1IrJUVbPcLtsP3Azc5eERLwD3qeoHItIH\nqHc79xNVfS0wkRtjjGmJgCUSYBaQrap7AETkZeAK4GQiUdUc1zn3JIGIjAdCVPUD13UVrQlk4MCB\nOnTo0NY8whhjepwNGzYcUtXY5q4LZCJJBHLd3ucBZ/l472igRETeAIYBHwJ3q2qd6/x9InIP8JHr\neJW3hw0dOpS0tLQWBW+MMT2diOzz5bpAjpGIh2O+buwVAszD6fKaCQzH6QID+Dkw1nW8P/Azjz9c\n5FYRSRORtOLi4haEbYwxpiUCmUjygGS390lAQQvu3aSqe1S1FngLmA6gqgfUUQU8i9OFdgZVfUJV\nU1U1NTa22ZaZMcYYPwUykawHRonIMBEJA64Blrbg3n4i0pABLsA1tiIi8a6vAiwBMto0amOMMS0S\nsETiakncCSwHtgGvqGqmiNwrIosBRGSmiOQBVwOPi0im6946nG6tj0QkHaeb7EnXo//PdSwdGAj8\nPlCfwRhjTPOkJ9QjSU1NVRtsN8aYlhGRDaqa2tx1trLdGGNMq1giMcYY0yqWSIwxnpUdgIzXOzoK\n0wVYIjHGeLb2UXjtm3BoV0dHYjo5SyTGGM8KM52vmW91bBym07NEYozxrNC1LV7mmx0bh+n0LJEY\nY8504iiUF0BMChRlQvHOjo7IdGKWSIwxZ2pojcz/CSCQZd1bpmmWSIwxZypyJZIRF0LKbOveMl5Z\nIjHGnKkwEyKiISoBJlzpJJbiHR0dlemkLJEYY85UlAWDJoAIjFsMiM3eMk2yRGKMOZ2qM0YSN8F5\nHxUPQ+Za95ZpkiUSY8zpSvZDdTnEjT91bPwSKN4GRds7Li7TaVkiMcacrmGgfdCEU8fGu7q3bPZW\n51dZCns/g88fgle/AZVlAf+RgazZbozpihpWtA8ad+pY38Ew5Gyne+u8uzsmLnOmqnI4sBUObIaC\nTc7rcPap8zEpUFYAEVEBDcMSiTHmdEVZEJ1y5i+fCUtg2V1QtO30JGPaR/VxOJh+KmEUbIJDOwFX\nTamoREiYBlOucb7GT4PIAe0SWkATiYgsBP4GBANPqer9jc7PBx4EJgPXqOprbudSgKdw6r4rcJmq\n5ojIMOBloD+wEbhBVasD+TmM6VEKs04fH2kwbjEs+4kze8sSSeDVVlG/8Z8EFWxwkkbxdtB651yf\nOEiYDhO/AglTIX4q9I3rsFADlkhEJBh4GFgA5AHrRWSpqma5XbYfuBmnrG5jLwD3qeoHItIHcP0J\n8gDwV1V9WUQeA74FPBqgj2FMz1JbDYd3wZhLzzzXNw6GnnOqe0uk/ePrIY4cqybjqduZf/R1jof2\nIyJlBkFjFzktjYRpzky6TiSQg+2zgGxV3eNqMbwMXOF+garmqOpWTiUJAERkPBCiqh+4rqtQ1eMi\nIsAFQEPL5XlgSQA/gzE9y6GdUF97aupvY+OvgEM7nO4t0+ZUlTc25nHbn57n7CNv8EHvyxhf/g8W\nHvo+G0bcAWMv63RJBAKbSBKBXLf3ea5jvhgNlIjIGyKySUT+6GrhDABKVLW2uWeKyK0ikiYiacXF\nxX5+BGN6mIYZW00lknGLQYJs9lYA7D98nBufWcePXtnMb0KeRXvFsODOh3n6pplUVNZy1WOr+fVb\nGZRX1nR0qGcIZCLx1O5VH+8NAebhdHnNBIbjdIH5/ExVfUJVU1U1NTY21scfa0wPV5gBQaEwYKTn\n833jTs3eUl//OhtvauvqefzT3Vz84Kds2l/CizP3Mb4mk5AFv4He/blwXBwrfnQuN80Zyj/X7mPB\nXz5jeebBjg77NIFMJHk4A+UNkoCCFty7ydUtVgu8BUwHDgExItIwttOSZxpjmlOYBbFjIDi06Wsm\nXOl0gRVlNX2N8Ul6XimL//E5f3hvO+eMjOXD705j3t6/OeMg0244eV2f8BB+s3gCb95xNjG9Q/nO\nixu47cUNFJZVdmD0pwQykawHRonIMBEJA64Blrbg3n4i0tCUuADIUlUF/gtc5Tp+E/B2G8ZsTM9W\nlAWDPMzYctfQvWV7b/nteHUtv38niyseXkVxRRWPXjedJ2+cweBNf4OKIrjszxAUfMZ9U5Nj+M/3\nzuEnl4zh4x1FXPTnT/nnF/uor+/Y1mHAEomrJXEnsBzYBryiqpkicq+ILAYQkZkikgdcDTwuIpmu\ne+twurU+EpF0nC6tJ12P/hnwIxHJxhkzeTpQn8GYHuXEUSjL9zz1112f2FOzt6x7q8U+3VnMxX/9\njKdW7eWaWSl8+KNzuXRSPFK8Hb54FKbfAEkzmrw/NDiI754/kuX/M59JSdH86q0MvvbEGrKLytvx\nU5xOtAf8j5CamqppaWkdHYYxndu+1fDspfD1V2H0xd6vTXsG3vkh3PY5DJ7YPvF1cYcqqvjdO1m8\nvbmAEbGR/OHLk5k1rL9zUhWe/5Kz4PB7G31eSKiqvLYhj9+/u40T1XXccf4Ibj9vBOEhZ7Zm/CEi\nG1Q1tbnrbK8tY4yjYWuU5lokAGO/5Oresh2Bm9Pwy/6iv3zKsvQDfP/CUSz7wbxTSQQg8w3IWQkX\n/KpFq9FFhKtTk/nox+eycOJgHvxwF5c/tIr1OUcC8EmaZonEGOMoynIVs/Jhln6fWBg6z5kG3AN6\nNfyVc+gY1z+9lrte3cKI2D4s+/48frRg9OkthqoKWP4rGDwZUr/p188Z2Cech66dxrPfmMmJ6jqu\nfmwNv3wznbJ2mipsicQY4yh0K2bliwlXOhsEFmYENq4uauWuYi558DO25pbyuyUTefU7cxgV1/fM\nCz/7f1BeAJd7HmBvifPHDGLFD+dzyznD+Ne6/Vz050/ZdiDwu/9aIjHGOK2Koib22GrKuC+BBFv3\nlgelx2u469UtpPTvzQc/OpcbZg8hKMhDgi7eCWsehqnXQfKsNvnZkeEh/GrReN767tlMS4lh6IDI\nNnmuN5ZIjDFQmgtVZc1P/XUXORCGzXOmAVv31ml+859MDlVU89evTWVwdITni1ThvZ9AaCRc9Js2\nj2FyUgyP35BKr7C2GXj3xhKJMcbp1oKmt0ZpyoQr4chuZ7aRAWB55kHe3JTPd88fycTE6KYv3LYU\n9nwC5/8C+gxqt/gCwRKJMQaKPBSz8sVY695yd+RYNb98M53x8VHceX4T28wAVB+D938BcRNh5i3t\nF2CAWCIxxjgtkuhkZ9ZWS0QOgGHzbfaWy6/fzqD0RA1//uoUwkK8/Hpd+Wcoy4PL/gjBXb++oCUS\nY4xvW6M0ZcKVcGQPHNzatjF1Me9sLeDdrQf4wYWjGBfvpbTt4d2w+u8w+WswZG77BRhAlkiM6elq\nq51NGFs6PtLAZm9RXF7Fr9/KYEpSNLedO6LpC1XhvZ9CcDgsuLf9AgwwSyTG9HSHd3kvZtWc3v1h\n+Lk9du8tVeWXb6ZzrLqOP109hZBgL79WdyyD7A+dCpN9B7dfkAFmicSYnq5haxR/u7bA6d46mgMH\ntrRJSF3J25sLWJFVyI8XjPa84LBBzQl4/26IHQdnfaf9AmwHlkiM6ekKM51iVgNH+f+MsYsgKKTH\ndW8VllVyz9sZzBjSj1vmDfd+8aq/Qsl+1wC7l3ovXZAlEmN6uqIsGDi6db/ceveHYT2re0tVufv1\nrVTX1fPHqyYT7GnleoMje2DVgzDxK84izm7GEokxPV1hC7dGacqEK6FkHxzY3PpndQGvbsjjvzuK\n+eklYxke28f7xe//3GmxXfz79gmunQU0kYjIQhHZISLZInK3h/PzRWSjiNSKyFWNztWJyGbXa6nb\n8edEZK/buamB/AzGdGsnSpz1DK0ZH2kw9vIe071VUHKC3/0ni7OG9efmuUO9X7zjfdj5Ppz7U4hK\naJf42lvAEomIBAMPA5cC44FrRaTx/637gZuBlzw84oSqTnW9Fjc69xO3cz3jnz/GBELRNuervzO2\n3PXuD8PP6/bdW6rKz17fSp0qf7xqiufNGBvUVML7P3O6Dmff0X5BtrNAtkhmAdmqukdVq4GXgSvc\nL1DVHFXdCtQHMA5jTFMatkZpi0QCru6t/VCwqW2e1wm9tG4/K3cd4ueXjSNlQG/vF69+yJnNdun/\ng5CwdomvIwQykSQCuW7v81zHfBUhImki8oWILGl07j4R2SoifxWR8FZHakxPVZgF4T4Ws/LF2Mud\nGWDdtHsr98hx7nt3G+eMHMj1Z6V4v7hkv7MVyvgrYMT57RNgBwlkIvHU3mtJezfFVSv468CDItKw\nXPTnwFhgJtAf+JnHHy5yqysRpRUXF7fgxxrTPXy8vZCvPr6G7KKKpi9qqEHiazGr5vTq5/zS7IZb\ny9fXK3e9uoUgER64ajLS3J/Z1n9DbWW3HWB3F8hEkgcku71PAgp8vVlVC1xf9wCfANNc7w+oowp4\nFqcLzdP9T6hqqqqmxsbG+vcJjOnClm4uYN3eI3z5kc9ZnX3ozAtUXVUR22Cg3d34JVC6H/I3tu1z\nO9gLa3JYu/cIv140jsSYXs3fkLPKqTgZ00zLpRsIZCJZD4wSkWEiEgZcAyxt5h4ARKRfQ5eViAwE\nzgayXO/jXV8FWAJYnU9jPEjPL2V6SgyDoyO48Zl1vLxu/+kXlOZBVWnbTP11N/Yyp3srq/t0b+09\ndIz739/O+WNi+WpqcvM31FbD/rXdcs2IJwFLJKpaC9wJLAe2Aa+oaqaI3CsiiwFEZKaI5AFXA4+L\niGvkj3FAmohsAf4L3K+qrso7/J+IpAPpwECg+7cbjWmhY1W17Dl0jHNHD+L12+dy9siB3P1GOve9\nm0VdvavLqcj1V2pQGw20N+jVD0Zc4F/3Vm21s83Kxhdh2U/g3R9DfV3bxtdCda4urbDgIP7wZR+6\ntAAKNkLtCRh6TuAD7AQCuhG+qi4DljU6do/b9+txurwa37camNTEMy9o4zCN6Xa2HShDFSYmRtE3\nIpSnb0rld+9k8eTKvew9dJy/XTOVyEI/i1n5YsIS2LUc8jdAUqrnayrLoDADDmx1tqA/uBWKtkN9\njXM+KMTZTDL1m203q8wPz6zay4Z9R/nLV6c0XTa3sZyVztchZwcusE6k61dUMcacIT2/FIBJrlKv\nIcFB/PaKiQwbGMm972Rx9WNreD0unV7RydArpu0DGHPZqdlbSalQUeRKGFtOJY4je05dHxkLgyfD\n3Iucr/FTnCTy8CxnrKWDEkl2UTl/XLGDBePjuHJaC2a27V3pVD/s3T9wwXUilkiM6YYy8suI7RvO\noKjT/wV989nDGDIwku+9tIm8kjTikkbhpQST/3rFwMgLYcNzkP4aVBw8dS5mCMRPhilfd74Onuxs\nqd64y6i+HsKjnFbN9BsCEaVXtXX1/PiVLUSGBfP/XTnJty4tgNoqyF0HM24KbICdiCUSY7qhzIJS\nJiZ4ThHnjxnE67fOZOiTBTy7bxopGQdZODEAtTHOus1piQwcfSphDJ7kewsoKAgSpjnjDR3gjY35\nbMkr5e/XTiO2bwuWq+U3jI/0jIF2sERiTLdTWVPHrqIKLh4f1+Q1Y0IOArVURI/mtn9u4GcLx3Lb\nucN9/1e3L0ac3/qFeInTnbK0NZUQ6uP4RBt5N/0AQwb0ZtHk+JbdmLMKkG5TRtcXtvuvMd3MtgNl\n1NUrE1zjIx65Btq/e80VLJoczwPvb+dnr2+luraT7VaUMN0ZKzmY3q4/tqyyhtW7D3HJhMEtT645\nPWt8BKxFYky3k+EaaJ/oLZEUZUJQCOFxY3jomlCGD4zkoY+z2X/kOI9dP4OY3v7tC1Vfr+w5VMHm\n3FK25JZQW6/8fslE77U6vEmc4Xwt2AjJM/17hh8+2VFMTZ16bdV5VFsFuWthxjcCE1gnZYnEmG4m\nI7+M/pFhJHibqlroKmYVEkYQ8KOLxzA8tg8/fW0rVz6ymqdvSm2+xgZOhcDNuSVsyS1hS14JW3NL\nKa+qBSAsOIjqunqumJrA7OED/PswUQnQJ84ZcG9HyzMPMrBPONNS+rXsxvwNzrYoPWT9SANLJMZ0\nMxkFpUxIiPLeJVOUBclnnXZoybREkvr14tYXN3DlI6t57PoZzBlxKgFUVNWyNa+ELa7Wxpa8Eg6U\nVgIQEiSMi4/iimkJTEmKYWqys6J+5n0f8u7WA/4nEhGnVdKO261U1dbxyfYiFk9NaHlLqgeOj4Al\nEmO6laraOnYWlvNtb/XDK0uhNNdZ6NdI6tD+vHXH2Xzz+fXc8PRavj1/OIfKq9iSV8KuooqTC9WH\nDujNrGH9mZIUw5TkGCYkRBERGnzG8y4YO4j3Mg7wm8UT/O/eSpgOO5Y5RbgCsealkdXZhzlWXcfF\nE/yYyZazEgb3rPERsERiTLey82AFNXXazPiI92JWKQN68/rtc7nzpY08+slu+keGMTU5hssnJTAl\nOZopSTH0i/RtDGXR5ASWpR9k7Z7DzB05sKUfx5E43fl6YLNTOCvAlmcepE94CHNHtLAV1bB+xEOC\n7u4skRjTjWQUuAbaE5qfseVt19/oXqG88M1ZHD5WzYDIML+nBZ8/ZhC9QoN5J/2A/4kkYZrzNX9j\nwBNJXb3y4bZCzhsTS3jImS0sr/LSeuT4CNj0X2O6lfT8UqIiQkju72Wb8yJXMavoM7a5O42IMLBP\neKvWlvQKC+bCcYN4P+MgtXV+Ti3u3R/6DWuXAfeN+49yqKKaS/zq1uqZ4yNgicSYbiUzv5SJidHe\nf/kXZjobNbbl4kMvFk1O4MixatbsOez/QxJntEv53hWZBwkLDuK8MX7UMMpZ6Vq538KZXt2AJRJj\nuomaunq2HSz3Pj7SUMyqrWuQeHHemFgiw4J5d+sB/x+SOB3K8qH8YPPX+klVWZ5ZyNyRA+gbEdqy\nm2sqIW99j9oWxZ0lEmO6iV2FFVTX1ntPJGX5TjGrtq6K6EVEaDAXjY/j/cyD1PjbvdWwMDGA04B3\nFJaz/8hxLh7vR7dWD10/0sASiTHdxKmBdi/7+Ra6ilm187bsiyYnUHK8hs89lfz1xeDJIMEBHSdZ\nnlGICFw0flDLb85ZiTM+MqfN4+oKAppIRGShiOwQkWwRudvD+fkislFEakXkqkbn6kRks+u11O34\nMBFZKyK7ROTfrjK+xvR4Gfml9AkPYeiAyKYvKmp+xlYgzBs1kL7hIf53b4X1dmIO4E7AyzMPMj2l\nH4P6+rE5ZM6qHjs+AgFMJCISDDwMXAqMB64Vkcb/9+4HbgZe8vCIE6o61fVa7Hb8AeCvqjoKOAp8\nq82DN6YLysgvZXxCFEHeFv4VZkFUUrss7HMXERrMgvFxLM886P/GkInTnK6tlpbv9UHukeNkHSjj\nkgkt3FsLnPGR3HUwbH6bx9VVBLJFMgvIVtU9qloNvAxc4X6Bquao6lbAp/+zxJmKcgHwmuvQ88CS\ntgvZmK6prl7JOlDmff0IOFN/23Gg3d2iKfGUVdayKrvYvwckzoDKktMrK7aRFVmFAH6Oj6RBXVWP\nHR+BwCaSRCDX7X2e65ivIkQkTUS+EJGGZDEAKFHV2uaeKSK3uu5PKy72839cY7qI3cUVVNbUMynJ\ny/hIXQ0U72j3bq0G54yMpW9ECO/4272V4FrhHoBpwCsyDzImri9DB3rpFmxKw/qRlJ45PgKBTSSe\n2tctaZOmqGoq8HXgQREZ0ZJnquoTqpqqqqmxsX7MCTemCzm5dby3FsmhXVBf02H1z8NCgrhkwmA+\nyCykqrau5Q8YNA5CItp8wP1wRRXrc474160FTn32+Mnt3l3YmQQykeQByW7vk4ACX29W1QLX1z3A\nJ8A04BAQIyINW7u06JnGdFcZ+WX0Cg32vvV7kWvGVge1SAAWTY6nvKqWz3b6MXsrOBTip7T5FOCP\nthdRr/i3SWMPXz/SIJCJZD0wyjXLKgy4BljazD0AiEg/EQl3fT8QOBvIUlUF/gs0zPC6CXi7zSM3\npotpGGj3usNuoVPMioGj2y+wRs4eOZDoXqG8u9XPf/8lTIcDW6CutvlrfbQi8yCJMb2Y4G3adFPy\n1rvGRyyRBIRrHONOYDmwDXhFVTNF5F4RWQwgIjNFJA+4GnhcRFxzExkHpInIFpzEcb+quv45xc+A\nH4lINs6YydOB+gzGdAX19UpmQan39SPgtEgGjIKQjpsxHxocxMIJg/kgq5DKGj+6txJnQO0JKN7W\nJvEcq6rls12HWDA+zr89xXJWgQRByuw2iaerCujuv6q6DFjW6Ng9bt+vx+meanzfamBSE8/cgzMj\nzBgD7D18jGPVdd5rtIMz9Te54//qLJoSz7/TcvlkRzELJ7awO6lhS/n8jc66jVb6bGcx1bX1/m3S\nCK71Iz17fARsZbsxXV7DQPskb4mksgxK93fY1F93c4YPoF/vUN5N92P2Vv/hEBHdZgPuyzMP0q93\nKDOH+rGQsOYE5K3r0dN+G1giMaaLyywoIywkiJGDvA20u7qCBnXMjC13IcFBLJwYz0fbCjlR3cLu\nLRFnnKQNVrjX1NXz0fYiLhwXR0iwH78K89ZDXXWPHx8BHxKJOK4XkXtc71NEpOPbx8YYANLzShk3\nuC+h3n4ZNmyN0glaJABfmhzP8eo6/rujqOU3J053uumqj7cqhi/2HKa8srZ13VoS1GP313LnSxp+\nBJgDXOt6X46z9YkxpoOpKhkFpd53/AVnxlZ4FEQne7+uncwa1p+BfcL823srcQZoHRxMb1UMKzIL\n6RUazLxRflZuzFnlTEeOaObPvgfwJZGcparfBSoBVPUoYBslGtMJ5B45QXllrQ+JJKtdi1k1x+ne\nGsxH2ws5VtXCqbwnV7j7371VX6+syDrIuaNjiQhtYUldcI2PrLfxERdfEkmNawNGBRCRWHzcG8sY\nE1jpvgy0qzpdWx24ENGTRZMTqKyp5+PtLezeioqHvgmtGnDfkldCYVkVF/u7mj13nY2PuPElkTwE\nvAkMEpH7gFXA/xfQqIwxPskoKCU0WBgV52WgvawAKks7bGuUpswc2p/YvuF+dm9Nb9UK9xVZhQQH\nCReO9TOR2PqR0zS7jkRV/09ENgAX4ux1tURV22Y1kDGmVTLySxkd15fwEC/dM0UdU8yqOcFBwuWT\n4vnXuv1UVNXSJ7wFy9oSp8P2d+DEUb9qgCzPPMjs4f2J7t3CkroNclZB/FQbH3HxZdbWbCBfVR9W\n1X8AeSJyVuBDM8Z4o6pk5Jd679YCZ6AdnDGSTubyyfFU1dbz0bbClt3Yip2As4sq2FN8zP/ZWtXH\nna3jbXzkJF+6th4FKtzeH3MdM8Z0oILSSo4er2l+RXtRFkQldsrqfTNS+jE4KqLlW8snTHO++jFO\nsjzzIAALxvvZrWXrR87gSyIR12aJAKhqPQHeWsUY07z0PB9qtINrxlbnGmhvEBQkXDYpnk93FFNW\nWeP7jb1iYMBIyG95i2RFViFTkqKJj+7V4nsBpz67BNv4iBtfEskeEfm+iIS6Xj8A2r5EmTGmRTIL\nSgkOEsbFN1fManunWYjoyeWT46muq+fDLD+6t1o4BfhgaSVbckv82zK+Qc4qSJgKEX7sFtxN+ZJI\nbgPmAvk4NUbOAm4NZFDGmOZl5JcyalAf7+sgDmc7xaw6wdYoTZmWHENCdETLZ28lzoDyA86sNB+t\nyHK6tfwuYlV9HPJsfKSxZhOJqhap6jWqOkhV41T166rqx74Gxpi2oqqk55cxobka7YWda2sUT4KC\nhMsnx/PZrmJKj7ege8t9J2AfrcgsZHhsJCMH9W1hlC5565zEbOMjp2kykYjIT11f/y4iDzV+tV+I\nxpjGisqrOFRRxaREH2qQdHAxK19cPjmBmjo92WLwyeBJzmfzccC99HgNX+w5zMXjW9mtJcGQbBNX\n3XlrkTSsFUkDNnh4NUtEForIDhHJFpG7PZyfLyIbRaRWRK7ycD5KRPJF5B9uxz5xPXOz6zXIl1iM\n6U5O1mj3ZWuUAaMgJLwdovLflKRokvr1atnW8qG9nEkEPo6TfLyjkNp69b9bC5z67DY+coYmZ1+p\n6n9cW6NMVNWftPTBrnsfBhbgjK2sF5GlbpUOAfYDNwN3NfGY3wGfejh+naqmtTQmY7qL9PxSRGB8\ns1URMyFpZvsE1QoiTvfW0yv3cvRYNf0ifdzOL3E6ZL4J9fUQ5L2nfnlGIXFR4UxJ8rMIVfUxp/Uz\n57v+3d+Nef2TV9U6YIafz54FZKvqHlWtBl4Grmj0/BxV3YqHvbtEZAYQB6zw8+cb021l5JcxIrYP\nvcO8zMSvKoeS/Z126m9jiyYlUFvfwu6txBnO9i9HvE8krayp49OdxSwYH0eQt7r23uTa+EhTfJm1\ntUlElorIDSLy5YaXD/clArlu7/Ncx5olIkHAn4GmWkLPurq1fi1+FVo2pmvLyPelRrurd7qTbY3S\nlImJUaT0792yxYk+7gS8ctchTtTU+b+aHU6Nj6TY+EhjviSS/sBh4ALgS67XIh/u8/QLXj0c8+QO\nYJmq5no4d52qTgLmuV43ePzhIreKSJqIpBUXF/v4Y43p/IrLqzhYVtn8+EhDvY4u0iIRERZNjmf1\n7sMcrqjy7abYsRDau9kB9+WZB+kbEcJZwwb4H2DOKmdFfbifM766MV8SyU9U9RuNXt/04b48wL2K\nThLg64TvOcCdIpID/Am4UUTuB1DVfNfXcuAlnC60M6jqE6qaqqqpsbGxPv5YYzq/zAIfBtprTsCa\nfzg1zmNS2imy1rt8cjx19coN02x+AAAgAElEQVTyTB8XJwaHOMWlvEwBrq1z9vK6cOwgwkL8rC7e\nMD5i60c88jb990siUgxsFZE8EZnbwmevB0aJyDARCQOuAZb6cqOqXqeqKao6FGcg/gVVvVtEQkRk\noCu+UJyWUUYL4zKmS2uYseV1oP2TPzjjBl/6W6cpZuWL8fFRDBsYybvpvi8yJHEGHNzqrOL3YH3O\nUY4er2ndavbctc74yDAbH/HEW3q+D5inqgnAV4A/tOTBqloL3Aksx5lK/IqqZorIvSKyGEBEZopI\nHnA18LiIZDbz2HBguYhsBTbjrLZ/siVxGdPVZeSXMWxgJFERTWyBXrAJVv8Dpt8Ew+a3b3Ct1NC9\ntWb3YYrLfezeSpgGtZWntstvZEXWQcJCgjh3dCt6Jk6uH7H9tTzxtvlirapuB1DVtSLS4o5BVV0G\nLGt07B6379fjdHl5e8ZzwHOu74/h/ywyY7qFjIJSpiY3MYW1rgbe/h5ExsKCe9s3sDZy+eR4/v5x\nNu9nHuSG2UOavyHR9Sshf6PTzeVGVVmRWcj8UQOJbEm9k8ZyVjlTjcO9FBDrwby1SAaJyI8aXh7e\nG2Pa2dFj1eQdPdH0+Mjqh6AwHRb9xdkhtwsaE9eXEbGRvLvVx+6tfkOhV3+PA+6ZBWXkl5xo3Wp2\nGx9plrdE8iTQ1+3V+L0xpp1lFpQBTdRoP7QLPnkAxi+BsZe3c2Rtx+neSmDt3iMUlVX6coPTveWh\nyNWKzIMECVw4rhUbYOSuhfpaSyReeFvZ/tv2DMQY07x010D7hMYD7fX1sPR7zrYhl/2xAyJrW5dP\njudvH+3ivYyD3DR3aPM3JM6AlX9yWg9hkScPL88sJHVofwb0acUWMXtXOnt62fhIk6xAlTFdSEZB\nKcn9exHTu9EWImlPw/41sORR6NP1t58bHdeX0XF9eGNT/plJ04Po0FGM0nq2b1pFRZyzJcyRY9Xs\nKCzn14tauY4mZ5Wz8NHGR5pkicSYLiQzv5SJjbeOL8mFD38DIy6AKdd2SFyBcMXURP64fAdXPbam\n2WtjqWF9BLy69G2erqs9eTxI4GJ/S+oCVFU4q+bnft//Z/QAzSYSEQlX1apGx/qr6pHAhWWMaays\nsoacw8e5OtVtna8qvPND5+uiB7vUmpHm3DJvGNNSYqir921DjMo34/lObCnnnXNqjfKAyHCS+/f2\nPwgbH/GJLy2SN0RkiarWAIhIPPAONg3XmHaVme8MtJ82Yyv9Vcj+ABY+AP18mCrbhYSHBDN3xEDf\nbxgyk4iD6Qwa1YY7WeSsco2P2P5a3viyX8BbwKsiEiwiQ3EWGP48kEEZY87UsDXKyTGDY4fgvZ9B\n0iyY9e0OjKyTSJwOR/fC8TbsLMlZ6Qzk2/iIV76U2n0S+AAnofwHuE1VbWt3Y9pZen4p8dERDGyY\ngfTez6C6Ahb/HYK81G3vKXzcCdhnVRXOIkfr1mpWk11bjRYdCs4GjJuB2SIyW1X/EujgjDGnZOSX\nnurW2vE+ZLwG5/8SBo3t2MA6i4SpgDi//Ede1Prn7f0UtM4SiQ+8jZE0XnT4ZhPHjTEBdqyqlj2H\njrF4SiJUljkD7IPGw9n/09GhdR4R0TBwlNedgH1WtA3evhOiU2z9iA9sQaIxXUDWgTJUneJPfPi/\nUHEQvvZPCPGxJG1PkTgDsj9yZrH5O4Pt8G54fjEEh8FNb0NYK2Z99RDNjpGIyAciEuP2vp+ILA9s\nWMYYdw1bx0/XLEh7BmbfAUk2cfIMCdPhWBGU5ft3/9F9ThLRerhpqVPPxTTLl1lbsapa0vBGVY8C\nXX/prDFdSHp+KUl9oN+HP3Y2KTz/Fx0dUufkvhNwS5UVwAuLobocbnwLYse0bWzdmC+JpE5ETpZY\nE5Eh+F4y1xjTBjLzy/h55FI4stspVuW2n5RxM3giBIU2W3r3DBXF8MIVcOwwXP8mDJ4UmPi6KV8S\nyS+BVSLyooi8CHyGj+tIRGShiOwQkWwRudvD+fkislFEakXkKg/no0QkX0T+4XZshoiku575kEg3\nWsprjAcnqusIK97KpaWvwrQbYPh5HR1S5xUSDnETWjYF+PgReHGJs9XMda9Yl6EffFlH8j4wHfi3\n6zVDVZsdIxGRYOBh4FJgPHCtiDTePW0/cDNO7XVPfgd82ujYo8CtwCjXa2FzsRjTlW0rOMz9IU9Q\nHdEfLv59R4fT+SXOgILNzo7Izaksg39+BQ7thGtfgiEtrShuwLcWCcBc4DzXy9e5cLOAbFXdo6rV\nwMvAFe4XqGqOqm4FzvgvLiIzgDhghduxeCBKVdeoqgIvAEt8jMeYrunzvzMhaB/HL3qgyxaraleJ\n06GqDA5ne7+u+hi89FWn3vtXX3A2vTR+8WXW1v3AD4As1+sHIuJL/fZEINftfZ7rWLNEJAj4M/AT\nD8/M8+eZxnRJh3YxKfsxPmA2/WZ8uaOj6RoaBty9dW/VVMLLX3c2ZfzKUzDm0vaJrZvypUVyGbBA\nVZ9R1WdwupJ8Kb/maezC10H6O4Blqprb6LjPzxSRW0UkTUTSiouLffyxxnQi9fWw9PtUEsbbCf+D\nDQf6aOBoCI1sesC9thpeuRH2fAJXPAITrmzX8LojX+uRxAANO6E1USz6DHk426o0SAJ8LMLMHGCe\niNwB9AHCRKQC+JvrOc0+U1WfAJ4ASE1NtVlmpuvZ+jLsX83vam8jJWVYR0fTdQQFO6V3PU0BrquF\nN26BXcvh8r/A1O5Tv6Uj+dIi+QOwSUSeE5HngQ2uY81ZD4wSkWEiEgZcAyz1JShVvU5VU1R1KHAX\n8IKq3q2qB4ByEZntmq11I/C2L880psvZ8R7VfZJ4pXbe6VvHm+YlTnPGPmqrTx2rr4e3vwtZb8PF\n98HMb3VcfN2ML7O2/oUzwP6G6zXHday5+2qBO3G2nd8GvKKqmSJyr4gsBhCRmSKSB1wNPC4imT7E\nfDvwFJAN7Abe8+EeY7oWVchdR37UFEDOrIpovEuYDnXVUOT6laIK7/7IaeWd/yuYe2fHxtfN+FIh\n8SNVvRC31oTbMa9UdRmwrNGxe9y+X8/pXVWenvEc8Jzb+zRgYnM/25gurTQXKg6SHj2aqIgQkvv3\n6uiIupaTK9w3QPxUWP4L2PAsnPNDmH9Xx8bWDXnbRj4C6A0MFJF+nBrojgIS2iE2Y3qu3HUAfHxs\nGBMTo22gvaViUqD3AMjfBGW/hy8egbNugwv/t1uVI+4svLVIvgP8D07S2MCpRFKGs9DQGBMouevQ\n0EiWFw/gxtHWrdViIk6rJOM1qK2E6TfCwvstiQRIk2Mkqvo3VR0G3KWqw1V1mOs1RVX/0dR9xpg2\nkLuW47FTOFEnTLCBdv8kTHeSyKSvwqIHLYkEkLeurZlArqr+3fX+RuArwD7gN6rahoWRjTEnVR+H\nwgz2DfsGABMbarSblplxM/TqBzNvsVLEAeZt1tbjQDU4mysC9+NsSVKKa32GMSYACjZBfS3Ly4Yw\nIDKMoQNsp1+/RMXD7Nsg2NflcsZf3v6Eg91aHV8DnlDV14HXRWRz4EMzpofKXQvA87mxfGvBUIKC\nrEvGdG7eWiTBItKQaC4EPnY7ZynemEDJW09haDI1YTHcOGdoR0djTLO8JYR/AZ+KyCHgBLASQERG\n4nRvGWPamip1+75gZeVErp8zhOjeoR0dkTHNajKRqOp9IvIREA+scG3bDk4r5nvtEZwxPc6RPQRX\nHmELo/neOba/lukavHZRqeoXHo7tDFw4xvRsZbtWEwUMHDefQVERHR2OMT7xtbCVMaYdZG/8iHLt\nxZKLrciS6ToskRjTSZQer6F34UbyIicyZGDfjg7HGJ9ZIjGmk/jXykxGsZ+BY8/p6FCMaRFLJMZ0\nAieq69i89iOCRYkdb4nEdC2WSIzpBF5ev59RVVkoAompHR2OMS1iicSYDlZTV8+Tn+3h/MgcJHYs\n9Irp6JCMaZGAJhIRWSgiO0QkW0Tu9nB+vohsFJFaEbnK7fgQEdkgIptFJFNEbnM794nrmZtdr0GB\n/AzGBNrbmws4UHqcSeyC5FkdHY4xLRawrU5EJBinbskCIA9YLyJLVTXL7bL9wM04ddndHQDmqmqV\niPQBMlz3FrjOX+eqlGhMl1Zfrzz26W4uii0jtLzUEonpkgLZIpkFZKvqHlWtBl4GrnC/QFVzVHUr\nUN/oeLWqVrnehgc4TmM6zIqsQrKLKrhj5GHnQJIlEtP1BPIXdCKQ6/Y+z3XMJyKSLCJbXc94wK01\nAvCsq1vr19JEDVIRuVVE0kQkrbi42J/4jQkoVeXRT7JJ6d+bKbrTqZ0xYGRHh2VMiwUykXj6Ba8e\njnmkqrmqOhkYCdwkInGuU9ep6iRgnut1QxP3P6GqqaqaGhsb28LQjQm81bsPsyWvlO+cO5yg/PWQ\nNBOCrPFtup5A/l+bByS7vU8CCpq4tkmulkgmTtJAVfNdX8uBl3C60Izpch75JJvYvuF8ZXxfKN5u\n4yOmywpkIlkPjBKRYSISBlwDLPXlRhFJEpFeru/7AWcDO0QkREQGuo6HAouAjIBEb0wAbckt4fPs\nw9xyzjAiDm50Dtr4iOmiApZIVLUWuBNYDmwDXlHVTBG5V0QWg1MXXkTygKuBx0Uk03X7OGCtiGwB\nPgX+pKrpOAPvy11jJ5uBfODJQH0GYwLl0U92ExURwnWzh0DeOpAgSJzR0WEZ45eAVjpU1WXAskbH\n7nH7fj1Ol1fj+z4AJns4fgywv22mS8suqmB51kHuPH8kfcJDnNK6cRMgvE9Hh2aMX2xkz5h29tin\nuwkPCeLmuUOhvg7yNli3lunSLJEY047yS07w1qZ8rpmZwoA+4VC0DarLIfmsjg7NGL9ZIjGmHT35\n2R4Avj1/uHMgb53zNXlmB0VkTOtZIjGmnRw5Vs3L6/dzxdREEmN6OQdz10NkLPSz+uym67JEYkw7\nee7zvVTV1nP7ecNPHcxd64yPeN6gwZguwRKJMe2goqqW51bncPH4OEYOcpXRPXYYjuy2hYimy7NE\nYkw7eGntPsoqa7n9PLe9tE6Oj1giMV2bJRJjAqyqto6nVu5l7ogBTE12K1qVuw6CQiBhWscFZ0wb\nsERiTIC9viGfovIq7jiv0c6+eeth8GQI7dUxgRnTRiyRGBNAtXX1PP7ZbiYnRXP2yAGnTtTVQv4G\n69Yy3YIlEmMC6L2Mg+w7fJw7zhvBaaVzCjOg5rglEtMtWCIxJkBUlUc+2c2I2EguHj/49JO5roF2\n2xrFdAOWSIwJkE92FrPtQBm3nTuCoKBG60Ty1kHfeIg+Y89SY7ocSyTGBMij/91NQnQEV0z1UGE6\nd63TrWULEU03YInEmABIyznCupwjfHv+cMJCGv01Ky+Ekv3WrWW6jYAmEhFZKCI7RCRbRO72cH6+\niGwUkVoRucrt+BAR2SAim0UkU0Ruczs3Q0TSXc98SMT+SWc6n0c+2U2/3qF8bWbymSdPLkS0HX9N\n9xCwRCIiwcDDwKXAeOBaERnf6LL9wM04tdfdHQDmqupU4CzgbhFJcJ17FLgVGOV6LQzIBzDGT9sO\nlPHx9iK+cfYweod5qB2XuxaCwyD+jNptxnRJgWyRzAKyVXWPqlYDLwNXuF+gqjmquhWob3S8WlWr\nXG/DG+IUkXggSlXXqKoCLwBLAvgZjGmxRz/ZTWRYMDfNGer5gtz1zmr2kPB2jcuYQAlkIkkEct3e\n57mO+UREkl212XOBB1S1wHV/nr/PNCbQ9h8+zjtbC7hu9hCie4eeeUFtNRRsgiSrP2K6j0AmEk9j\nF+rrzaqaq6qTgZHATSIS15JnisitIpImImnFxcW+/lhjWuXxz3YTEhTEt85por7Iwa1QV2ULEU23\nEshEkge4jzQmAQUtfYirJZIJzHM9033ifZPPVNUnVDVVVVNjY2Nb+mONabGi8kpe3ZDHV2YkERcV\n4fkiW4houqFAJpL1wCgRGSYiYcA1wFJfbhSRJBHp5fq+H3A2sENVDwDlIjLbNVvrRuDtwIRvTMs8\nvWovtXX13Hbu8KYvyl0L0SkQFd9+gRkTYAFLJKpaC9wJLAe2Aa+oaqaI3CsiiwFEZKaI5AFXA4+L\nSKbr9nHAWhHZAnwK/ElV013nbgeeArKB3cB7gfoMxviq9EQN//fFfi6fnMCQAZFNX5i33rq1TLfj\nYW5i21HVZcCyRsfucft+Pad3VTUc/wDwODdSVdOAiW0bqTGt8+KaHCqqar23RkrzoCzfEonpdmxl\nuzGtdKK6jmc+z+G8MbFMSIhu+sJcq4houidLJMazmkr44B544ztQc6Kjo+nU/r1+P0eOVZ9ZuKqx\n3HUQ0gvirEFtupeAdm2ZLqowE17/NhRlAuJ0x3z93xDmpe+/h6qpq+fJlXtJHdKPWcP6e784bx0k\nTodgD+tLjOnCrEViTqmvhzUPwxPnwbFiuO41+PITsO9z+OdXoLKsoyPsdJZuLiC/5AR3nD/C+4U1\nJ+DAVuvWMt2StUiMo6wA3rwN9n4KYy6HxQ9B5EDnXHAovH4LvHglXP869Irp2Fg7ifp65dFPdzN2\ncF/OHzPI+8UFm6G+xtaPmG7JEok3K34FR/fBqAUw8iKISmj+nq4o8y34zw+grhq+9DeYftPpdTIm\nXOlsMvjKTfDCYrjhLejdTDdOD/DBtkKyiyr42zVTaXYT6jwbaDfdlyUSb4LDIS8NtrnWUQ6aAKMu\ngpELIGV21+/rriyD934GW16CxBnw5SdhQBNdNGMvh2v/BS9fB88tghvfhj49d8eAhjK6Kf17c/kk\nHxYX5q6D/sNPtfKM6UZsjMSbC38NP8qC21fDgnudf4WveQSeXwQPDHN+qaY966wP6Gr2fwGPnQNb\nX4b5P4VvLm86iTQYtQCuewWO7IHnLofyg+0Ta0c5vBvSX3PGjhpZs/swW3JLuHX+cEKCm/lrpOok\nEuvWMt2UtUiaIwJxE5zX2T+AqnLY8ylkfwC7PoTt7zjXxY5ztVYugpQ5nXeL8Loa+PQBWPlniE6G\nb7wPKS0osDT8PGec5KWvwrOXwk3/6Z51x48fgReXOJUMNzwHSx6FmFNbxz3yyW5i+4Zz1QwfPvvR\nHDhWZN1aptuyRNJS4X1h3CLnpQrFO1xJ5QP44jFY/XcIjYTh5zpJZdA4qK10Zu3UnPDw/XFnzUbt\nCedrzfHTrwnv69SuaHhFJ/lf5/tQNrzxbSjYCFOvh4V/gIiolj9n6Nlww5vOTK5nL3OSSb8h/sXU\nGdXXw5vfgbIDMP8n8MWj8OhcuPT/wZRr2JpfyqrsQ9x96VgiQoObf17eeuerJRLTTVkiaQ0RGDTW\nec39HlRVwN7PTrVWdixr/hlBoRDay3mFRLh938v5JX/sEKx+COprnet7Dzw9sSRMa34DQFXnX9XL\nf+EMml/9PExoZT2w5FnOOMmLV7qSydLmu8a6ipV/hl0r4LI/waxvw7Tr4c3b4a3bYMe7vHDiG0RF\nhHDdWSm+PS93HYT1gUGNC4Qa0z2IU2iwe0tNTdW0tLT2/aGqcGgXlOU5SeG0ZNEbQiOc48E+5PKa\nSmdxYMEm55W/CYq3gbr67vvGn55Y4qeeGgg/dgiWfs9JasPPc7po2nL22YGtThdQUKjTMokd3XbP\n7gi7P4YXvwyTrnImHzS0/urrYM3D6Ee/43BdBJ+N/V++fO0tvj3zsXnQq5+TbI3pQkRkg6qmNnud\nJZIuqvo4HEw/lVwKNsGhnZys8xWdDPFTnH8NV5bCRb+Bs26DoADMryjaBs8vdn72jUshrov+y7s0\nDx6fD5GD4NsfeVzJ/+cX3+Cy7N8wTvbBtBuc7sHwvk0/s6oC7k+BeT+GC34ZwOCNaXu+JhLr2uqq\nwno7g+TuA+WVZU4FvpMtl40Qk+IsLoybELhYBo2DbyyD57/kzOa68S0niXUltdXOOpnaKvjaix6T\nSEHJCR7dFsHxWc/z6z5L4fMHnQWcVz4OQ+Z6fm7BRtA6Gx8x3Zolku4kIgqGnuO82tvAUa5ksthJ\nKNe/CUkz2j8Of634FeSnOeNHA0d5vOTJlXsA+Ma5Y6Df/8Lohc6g/LOXOWNkF/zqzNl6JysiNvuP\nOmO6LFtH4sXH2wt5L/0ABSUn6GpdgKrK5twSPttZTF19O8Xef7iTTHr1gxeucNaqdAXpr8G6x2H2\nd5uchHDkWDUvr8tl8dQEkvr1dg6mnAW3rYIZNzsTIp44z+ludJe3HgaOcf5MjOmmAtoiEZGFwN+A\nYOApVb2/0fn5wIM4RayuUdXXXMenAo8CUUAdcJ+q/tt17jngXKDU9ZibVXVzIOJ/4rM9fLHnCACx\nfcOZkhTD1ORopib3Y1JSNNG9Ot/K9sqaOv6zpYAX1uwjPd/5I0rq14sbZg/hq6nJ9IsMC2wAMSlw\n8zJnK5UXvwyX/wnCo5wpzQ2vmsom3lc506Brq1zTo6ucacUL7w/civCi7c5khOTZsOC3TV723Od7\nOVFTx+3nNpqZFt4HvvSgs/L/7e/CE+fD+b9w1hxJkNMiGXtZYGI3ppMI2GC7iAQDO4EFQB5ODfdr\nVTXL7ZqhOMniLmCpWyIZDaiq7hKRBGADME5VS1yJ5J2Ga33h72B7VW0d2w+UsyWvhM25zmtP8bGT\n54fHRjI1KYYpyTFMTY5hbHxfwkN8WFcQAPklJ/jnF/t4ed1+jh6vYeSgPtw0Zwj9IsN4Yc0+1u09\nQnhIEEumJnLj3CHeCzC1hfJCJ5kUb/dykbhmsUU4XxteJ9+Hw741zo4CVz3T9DiEv6rK4ckL4MRR\n+M7KJqdRV1TVMvcPHzF7+ACeuNFLF9XxI/DODyHrLUg+C+bdBS9dDYv/DtNvbNvYjWkHnWGwfRaQ\nrap7XAG9DFwBnEwkqprjOnfaHhSqutPt+wIRKQJigZIAxnuG8JBgpiQ7ieLGOc6x0hM1pOeVsiWv\nhE37S/hs1yHe2JQPQFhwEOMSopiaFH3yvmEDIgkK8nMBYTNUlTV7DvP86hw+yCoE4KJxcdw0dyhz\nRww4uZHgoskJbDtQxgtr9vHmpjz+nZbLzKH9uHHOUBZOHExoc1t8+KNvHHz7v1CY4SQET4kiOKz5\nxZUHtsKrNzn7e13wSzj7h20z80zVaYkcznZmmnlZi/PS2n2UVdZyx/nNFK7q3R+ufs7pKlv2YyeJ\ngG2NYrq9QLZIrgIWquotrvc3AGep6p0ern2OJloZIjILeB6YoKr1rmvnAFXAR8Ddqlrl4b5bgVsB\nUlJSZuzbt6+tPtppVJUDpZVsyT3VaknPL+V4dR0AUREhTlJxtVymJEczqG9Eq37msapa3tyUzwtr\ncthZWEFM71CumZnC9bNTTvXfN6H0eA2vbsjlhTX72H/kOIP6hnPdWUO49qzkVscVMJVl8J/vQ+ab\nzm4BVz4BkQO83rL/8HHuWZrBhpyjjE+IYmpyzMnknhAdgax9DN6/25kWfc4Pm3xOVW0d8x74LyMH\n9eGlb8/2PebSfCdRlebCHWsDM+3amADr8HUkInI1cEmjRDJLVb/n4drn8JBIRCQe+AS4SVW/cDt2\nEAgDngB2q+q93mJp73UkdfVKdlEFm3OPsjm3lC25JewoLD856J0Y04spydEnk8ukxGgiw5tvHO49\ndIwX1+zj1Q25lFfWMiEhipvmDmXxlATftupoFOOnO4t4fvU+Pt1ZTGiwcNmkeG6cM5TpKTHNb4ve\n3lQh7Wl4/+fO6v6rn3V2YG6ktq6eZz7fy18+2ElIUBCXThzMzqIKthWUUV3nNHwvjNzL43X3sH/A\nOeRf8hSTk/s1Od710tr9/OLNdP75rbM4Z5Qf4zSq/m9pY0wH6wyJZA7wG1W9xPX+5wCq+gcP1z5H\no0QiIlE4SeQPqvpqEz/jPOAuVV3kLZbOsCDxRHUdmQWlJ1stW/JKyD3i1EIPEhgd1/dkYpmaHMPo\nuD6EBAdRX698urOY59fk8MmOYkKCnF/4N81tu1/4e4orePGLfbyWlkd5VS2TEqO5cc4QvuRHggq4\nA1uc9R4l++HCe2Du90/+az89r5S739hKZkEZF42L43dLJhAf3Qs4Nd61Y/duLln1VSrqQrj0xO8o\nw1kv4j7eNSU5hnHxfQkJCuKCP39CVEQoS+88u/MlV2MCrDMkkhCcwfYLgXycwfavq2qmh2ufwy2R\niEgY8B7wH1V9sNG18ap6QJy/1X8FKlX1bm+xdIZE4snhiiq25pWyKbeELa7kUnK8BoCI0CAmJUZT\nXF5FzuHjxPYN57qzUvj6rBQGRQWmC6qioctsdQ67iiro1zuUc0fHNr9Nuhfx0RF8bWZys11uLVJZ\n6nQbZb0Noy7h+OX/4C+rDvHM53sZ0CecexdPYOHEwWf+4q+rdbZzyVsPt3xIafTYk+NdDQm+uNzp\nJQ0LDiJlQG+yiyp49LrpXOpLzRFjupkOTySuIC7Dmd4bDDyjqveJyL1AmqouFZGZwJtAP6ASOKiq\nE0TkeuBZwD3p3Kyqm0XkY5yBdwE2A7epaoW3ODprImlMVdl/5LjTYsktZXPuUcJCgvj6WUNYOGEw\nYSHt08+uqqzZfZjn1+SQke9/nXZVpbC8ClX1OAmglUHC+qeof//nFGs0t1feyZiZF3H3pWObnpb9\n4W9h1V/gikdg2nUe4z053pXnJPfIsBCevDE1YBMmjOnMOkUi6Sy6SiLpjgpKTvB/a/fxr3W5HDlW\nfXJa8pXTk+jjw7hQUw5VVPG7d7LYvWUVT0T8ncEcJuii/3VWmHtKVDveg39d45QRXvxQKz6RMT2H\nJRI3lkg6XmVNHe9uPcDza3LYmldK3/AQvjIjiRvmDGFEbB+fn6OqvL4xn9+/m8WxqlpuP28k350z\nkPB3vw/b/gOjL4Ulj5xeU/7IXnj8XOg/FL65wpl+bIxpliUSN5ZIOpdN+4/ywpp9vLO1gJo6Zd6o\ngdw8dyjnjRlEsJcupH2Hj/GLN9P5PPswM4b04/4vT2JUnGvnXVVY9wQs/yX0HQxXPQvJM50V8k8v\ncAbnv/MZ9BvaPh/SmLttTuQAAAjmSURBVG7AEokbSySdU3F5FS+v288/1+6jsKyKlP69uWH2EK5O\nTSKm96mtXGrq6nlq5V4e/HAnYcFB/PTSsVw3K8XzuEX+Bnj1ZigrgIt+69Rt2fRP+PorMPqS9vtw\nxnQDlkjcWCLp3Grq6lmRWcjza3JYt/cIEaGurVzmDKWmrp6730hn24EyLpkQx28XT2RwdDNdUyeO\nwtt3wvZ3nPfzf+LszGuMaRFLJG4skXQdWQVlvPhFDm9uyqeypt6pZtw3nN8unsjCiYN9f5BrVheH\ndjnFp4I62XoYY7oASyRuLJF0PQ1buZRX1vKtecOIiuh8Oy0b0911hk0bjfFbdO9Qbpk3vKPDMMb4\nwHaSM8YY0yqWSIwxxrSKJRJjjDGtYonEGGNMq1giMcYY0yqWSIwxxrSKJRJjjDGtYonEGGNMq/SI\nle0iUgzs8/P2gcChNgynrVl8rWPxtY7F1zqdPb4hqhrb3EU9IpG0hoik+bJFQEex+FrH4msdi691\nOnt8vrKuLWOMMa1iicQYY0yrWCJp3hMdHUAzLL7Wsfhax+Jrnc4en09sjMQYY0yrWIvEGGNMq1gi\ncRGRhSKyQ0SyReRuD+fDReTfrvNrRWRoO8aWLCL/FZFtIpIpIj/wcM15IlIqIptdr3vaKz7Xz88R\nkXTXzz6jipg4HnL9+W0VkentGNsYtz+XzSJSJiL/0+iadv3zE5FnRKRIRDLcjvUXkQ9EZJfra7//\nv72zjbWjKOP4728LGqWW0gJW0GgbNCBSbKChIKQKqdA0VAmRGhIbIRFUMHwwSkKiFb+ArzFKNBEJ\naACrQrFBCCViWtSU3rS0pbykb5JY+oIBUywaEfr4YebY9fTsuefePbt7E/+/ZHNmZ57Zec6zM+c5\nM7M7U5J3WZbZLmlZg/p9S9Jz+f6tlHRsSd6+daFG/ZZLeqFwDxeV5O3b1mvUb0VBt+clbSrJW7v9\nhk5E/N8fwCRgJzALOBrYDJzWJfN54Mc5vBRY0aB+M4G5OTwF2NZDvwXAgy3a8HlgRp/0RcDDgIBz\ngCdavNf7SM/Ht2Y/4AJgLrC1EPdN4MYcvhG4tUe+44Bd+XNaDk9rSL+FwOQcvrWXfoPUhRr1Ww58\naYD737et16VfV/p3gK+2Zb9hH+6RJOYBOyJiV0S8BvwCWNIlswS4K4d/DVwoSU0oFxF7I2JjDv8d\neBY4qYmyh8gS4GeRWAccK2lmC3pcCOyMiPG+oDoUImIt8HJXdLGO3QV8vEfWjwGPRsTLEfE34FHg\n4ib0i4jVEfF6Pl0HnDzscgelxH6DMEhbr0w//fLvxieBe4ddblvYkSROAv5SON/NkT/U/5XJjekA\nML0R7QrkIbUPAU/0SJ4vabOkhyV9oFHFIIDVkjZI+myP9EFs3ARLKW/AbdoP4MSI2AvpzwNwQg+Z\niWLHq0g9zF6MVhfq5Lo89HZHydDgRLDf+cD+iNhekt6m/caFHUmiV8+i+3G2QWRqRdIxwH3ADRHx\nSlfyRtJwzRzgB8ADTeoGnBcRc4FLgC9IuqArfSLY72jgUuBXPZLbtt+gTAQ73gS8DtxdIjJaXaiL\nHwGzgTOBvaTho25atx/wKfr3Rtqy37ixI0nsBt5VOD8Z2FMmI2kyMJXxda3HhaSjSE7k7oi4vzs9\nIl6JiIM5/BBwlKQZTekXEXvy54vAStIQQpFBbFw3lwAbI2J/d0Lb9svs7wz35c8Xe8i0asc8ub8Y\nuDLygH43A9SFWoiI/RHxRkQcAn5SUm7b9psMXAasKJNpy35VsCNJjACnSHpv/te6FFjVJbMK6Dwh\ncznwWFlDGjZ5TPWnwLMR8d0SmXd05mwkzSPd25ca0u9tkqZ0wqRJ2a1dYquAT+ent84BDnSGcRqk\n9J9gm/YrUKxjy4Df9JB5BFgoaVoeulmY42pH0sXAV4BLI+IfJTKD1IW69CvOuX2ipNxB2nqdXAQ8\nFxG7eyW2ab9KtD3bP1EO0lNF20hPdNyU424mNRqAt5CGRHYA64FZDer2YVL3ewuwKR+LgGuBa7PM\ndcDTpKdQ1gHnNqjfrFzu5qxDx35F/QTclu37FHBWw/f3rSTHMLUQ15r9SA5tL/Bv0r/kq0lzbr8D\ntufP47LsWcDthbxX5Xq4A/hMg/rtIM0vdOpg5ynGdwIP9asLDen381y3tpCcw8xu/fL5EW29Cf1y\n/J2dOleQbdx+wz78ZrsxxphKeGjLGGNMJexIjDHGVMKOxBhjTCXsSIwxxlTCjsQYY0wl7EiMyUia\nXliddV/XSrJ/qqG8zorDTyqt7Py1cVxjTHpJulPS5WMtx5h+TG5bAWMmChHxEml5DSQtBw5GxLdr\nLvbxiFicXz7bJOnBiNgwWiZJkyK9xX1uzfoZMyrukRgzAJIO5s8FktZI+qWkbZJukXSlpPV5D4nZ\nWe54SfdJGsnHef2uHxGvAhuA2ZImKe39MZIXILymUPbvJd1DevGuqJdynq1ZjysK8T+U9Iyk39J7\nIUhjKuEeiTFjZw5wKmmttV2kt87nKW04dj1wA/B94HsR8QdJ7yYtY3Jq2QUlTSft0/IN0lvaByLi\nbElvBv4oaXUWnQecHhF/7rrEZaTe1BxgBjAiaS0wH3g/8EHgROAZ4I6qBjCmiB2JMWNnJPI6YZJ2\nAp0f+aeAj+TwRcBpOrxlzdslTYm0n0yR8yU9CRwCbomIpyV9HTijMJcxFTgFeA1Y38OJQFpG596I\neIO0+OMa4GzSBkud+D2SHqv21Y05EjsSY8bOvwrhQ4XzQxxuU28C5kfEP0e51uMRsbgrTsD1EfE/\nizFKWgC8WnKdfpuseR0kUyueIzGmHlaTFoIEQNKZY8j7CPC5vHUAkt6XJ+P7sRa4Is+vHE/qiazP\n8Utz/EwO95iMGRrukRhTD18EbpO0hdTO1pJWGx6E24H3ABvz0vZ/pfe2u0VWkuZDNpN6IF+OiH2S\nVgIfJQ27bQPWjPF7GDMqXv3XGGNMJTy0ZYwxphJ2JMYYYyphR2KMMaYSdiTGGGMqYUdijDGmEnYk\nxhhjKmFHYowxphJ2JMYYYyrxH/jURoXRATTDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f925ed7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(testY[20:40],label=\"v\")\n",
    "plt.plot(test_predict[20:40],label=\"p\")\n",
    "plt.xlabel(\"Time Period\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
